{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Covid-19 Analysis\n",
    "---\n",
    "Gruppo 3:\n",
    "\n",
    "- Alessio Contin matr. 734792\n",
    "- Stefano Santini matr. 726396\n",
    "\n",
    "Laboratorio di Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Table of contents\n",
    "1. [Introduzione](#introduction)\n",
    "2. [Province](#province)\n",
    "3. [Regioni](#regioni)\n",
    "    1. [Analisi e visualizzazione](#analysis)\n",
    "    2. [PCA](#pca)\n",
    "    3. [LAG](#lag)\n",
    "    4. [Autocorrelation e Partial Autocorrelation](#autocorrelation)\n",
    "    5. [Time Series](#ts)\n",
    "    6. [Clustering](#clustering)\n",
    "        1. [K-Means](#kmeans)\n",
    "        2. [DBSCAN](#dbscan)\n",
    "    7. [Regressione](#regression)\n",
    "    8. [Forecasting](#forecasting)\n",
    "    9. [Modello logistico](#logistic)\n",
    "    10. [Modello SIR](#sir)\n",
    "    11. [Modello SEIR](#seir)\n",
    "    12. [Modello SEIR con distanziamento sociale](#seird)\n",
    "    13. [SARIMAX](#arima)\n",
    "    14. [Altri modelli per forecasting](#otherforecasting)\n",
    "4. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le domande a cui vorremmo dare risposta sono le seguenti:\n",
    "1. quali sono le province più colpite in base al numero di contagi?\n",
    "2. quali sono le regioni più colpite? (valutazione di cosa si intende con maggiormente colpita sulla base dei vari indici epidemiologici)\n",
    "3. analizzare l'andamento nel tempo delle diverse features per ogni regione\n",
    "4. forecasting per valutare l'andamento delle features, ad esempio quando il numero di nuovi contagiati sarà sotto una certa soglia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduzione <a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inizialmente importiamo tutte le librerie necessarie all'esecuzione dei successivi codici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importazione delle librerie\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import math\n",
    "import datetime as dt\n",
    "import plotly.express as px\n",
    "\n",
    "%matplotlib inline\n",
    "%run -i \"ts.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from dateutil.parser import parse \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from scipy import interpolate\n",
    "\n",
    "#from fbprophet import Prophet\n",
    "#from fbprophet.plot import plot_plotly, add_changepoints_to_plot\n",
    "import plotly.offline as py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dataset impiegati sono quelli di Kaggle prelevati in data 23/03/2020, relativi all'Italia, uniti ad un dataset creato, sfruttando le infomrazioni presenti nei seguenti siti:\n",
    "\n",
    "Attributo realtivo alla popolazione--> https://www.tuttitalia.it/regioni/popolazione/\n",
    "\n",
    "Attributo relativo ai posti in terapia intensiva--> http://www.quotidianosanita.it/studi-e-analisi/articolo.php?articolo_id=82888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Province <a name=\"province\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importiamo il dataset relativo alle province ed effettuiamo le prime analisi e visualizzazioni relative ad esso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carico il dataset \n",
    "province_italy=pd.read_csv('covid19_italy_province.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizziamo informazioni sulle colonne e sui loro tipi\n",
    "province_italy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizziamo come è formato il dataset come numero di righe e colonne\n",
    "province_italy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per avere una prima impressione sul contenuto del dataset visualizziamo alcune righe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizziamo le prime cinque righe\n",
    "province_italy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizziamo le ultime cinque righe\n",
    "province_italy.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da una prima esplorazione del dataset, tramite le funzioni head() e tail(), notiamo che vi sono alcuni missing values, codificati come NaN nella colonna ProvinceAbbreviation, e come 'In fase di definizione/aggiornamento)' nella colonna ProvinceName."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per avere più informazioni proviamo ad invocare la funzione describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "province_italy.ProvinceName.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invocando la funzione describe sulla feature ProvinceName notiamo che ha 108 valori univoci, ovvero i nomi delle 107 province, più il valore che consideriamo come nullo 'In fase di definizione/aggiornamento'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizziamo la funzione describe anche sull'altra feature che contiene valori nulli, ovvero 'Province Abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "province_italy.ProvinceAbbreviation.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo che vi sono 106 valori univoci, invece di 107, numero delle province. Indaghiamo ulteriormente visualizzando la parte di dataset che ha ProvinceAbbreviation nullo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "province_italy[province_italy.ProvinceAbbreviation.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo nella riga 5 che abbiamo l'attributo ProvinceName che ha come valore Napoli, mentre la rispettiva ProvinceAbbreviation risulta nulla. Questo perchè pandas ritiene che NA non sia l'abbreviazione di Napoli ma l'indicazione di un valore nullo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gestiamo il problema dei valori nulli e dell'abbreviazione di Napoli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp=province_italy\n",
    "mask = province_italy.ProvinceName == 'Napoli'\n",
    "column_name = 'ProvinceAbbreviation'\n",
    "province_italy.loc[mask, column_name] = 'NAP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostriamo come il dataset modificato adesso nella colonna relativa all'abbreviazione di Napoli non mostri più valore nullo ma il valore NAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "province_italy[province_italy.ProvinceAbbreviation == 'NAP'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prima di eliminare i nulli dal dataframe originario, li copiamo su un\n",
    "#dataset temporaneo, nel caso ci servissero per qualche analisi successiva\n",
    "temp=province_italy[province_italy['ProvinceAbbreviation'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo deciso di rimuovere i valori nulli, in quanto i relativi valori di TotalPositiveCases variano molto, indicando che i valori che erano in fase di aggiornamento, in parte nei giorni successivi vengono attribuiti alle rispettive province."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rimuoviamo i valori nulli\n",
    "province_italy.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifichiamo che non ci siano più valori nulli\n",
    "province_italy[province_italy.ProvinceAbbreviation.isnull()].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo verificato che non ci siano più valori nulli, adesso vediamo le prime informazioni sul numero totale di positivi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "province_italy.TotalPositiveCases.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per visualizzare meglio i dati, memorizziamo in un dataframe provvisorio i dati relativi all'ultima giornata presente, in modo da visualizzare il totale dei positivi corrente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDf=province_italy[province_italy.Date == province_italy.Date.max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordiniamo il dataframe per il totale dei casi positivi, in ordine decrescente, in modo da selezionare la top 10 delle province per numero di contagiati attuali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordiniamo il dataframe\n",
    "topDf=currentDf.sort_values(by='TotalPositiveCases', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selezioniamo la top10 delle province\n",
    "topDf=topDf.iloc[:11,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizziamo la top 10 delle province\n",
    "topDf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per visualizzare meglio i dati, li proponiamo in formato visivo attraverso l'uso di grafici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.barplot('ProvinceAbbreviation', 'TotalPositiveCases', data=topDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalla visualizzazione e analisi precedente notiamo che la maggior parte dei casi sono in Lombardia,Piemonte e Veneto, con alcuni anche in Marche ed Emilia Romagna. Per dare una visione globale, in rapporto anche alle province di altre regioni, diamo una visualizzazione grafica su una mappa interattiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizziamo su una mappa, utilizzando la libreria plotly\n",
    "fig = px.scatter_geo(currentDf, lat='Latitude', lon='Longitude',\n",
    "                     hover_name=\"ProvinceAbbreviation\", size=\"TotalPositiveCases\",\n",
    "                     projection=\"natural earth\", scope='europe', color=\"TotalPositiveCases\", center={'lat':41.902782, 'lon':12.496366})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dal dataframe precedente, contenente le top 10 province all'ultima data disponibile, ricaviamo le loro abbreviazioni "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinces=[]\n",
    "for i in topDf.ProvinceAbbreviation:\n",
    "    provinces.append(i)\n",
    "print(provinces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizzando le abbreviazioni, partendo dal dataframe originale, otteniamo un nuovo dataframe contenente le informazioni relative a ciascuna delle province presenti nella top 10 per ogni giorno presente nel dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nuovo dataframe per top 10 province con arco temporale completo\n",
    "topDfOverTime=province_italy[province_italy.ProvinceAbbreviation.isin(provinces)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostriamo tramite grafici l'andamento del numero totale positivi nel corso dell'intero arco temporale presente per le province maggiormente colpite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grafico andamento\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "topDfOverTime.groupby(['Date','ProvinceAbbreviation'])['TotalPositiveCases'].max().unstack().plot(ax=ax, marker= 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizziamo delle boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.boxplot('ProvinceAbbreviation', 'TotalPositiveCases', data=topDfOverTime)\n",
    "plt.xlabel('ProvinceAbbreviation') # Set text for the x axis\n",
    "plt.ylabel('TotalPositiveCases')# Set text for y axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per dare una visione globale, in rapporto anche alle province di altre regioni nell'arco temporale completo, diamo una visualizzazione grafica su una mappa interattiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mappa interattiva che mostra diffusione durante tutto arco temporale\n",
    "fig = px.scatter_geo(province_italy, lat='Latitude', lon='Longitude', \n",
    "                     hover_name=\"ProvinceAbbreviation\", size=\"TotalPositiveCases\", animation_frame=\"Date\",\n",
    "                     projection=\"natural earth\", scope='europe', color=\"TotalPositiveCases\", center={'lat':41.902782, 'lon':12.496366})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Diamo un'occhiata all'andamento della provincia più colpita, ovvero Bergamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creiamo un dataframe con i dati relativi a Bergamo\n",
    "BG_df=province_italy[province_italy.ProvinceAbbreviation == 'BG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per capire meglio l'andamento del contagio, aggiungiamo una colonna che indica il numero del giorno dall'inizio dell'epidemia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateCount=province_italy[province_italy.ProvinceAbbreviation == 'BG'].Date.count()\n",
    "ticks=range(0,dateCount,1)\n",
    "BG_df=BG_df.sort_values(by='Date', ascending=True)\n",
    "BG_df['DayNumber']=ticks\n",
    "BG_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizziamo l'andamento tramite grafici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#province_italy[province_italy.ProvinceAbbreviation == 'BG']\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "g=sns.barplot('DayNumber', 'TotalPositiveCases', data=BG_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(data=prova, palette=\"tab10\", linewidth=2.5)\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.lineplot('DayNumber', 'TotalPositiveCases', data=BG_df, marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo che l'andamento segue una curva molto accentuata, quasi esponenziale, con un numero di contagi superiore anche a città più grandi come ad esempio Milano. Resta da capire perchè il numero di contagi si è concentrato in Lombardia, se c'è stato qualche evento particolare, o se c'è qualche relazione anche in base a parametri e risultati che vedremo nella sezione sulle regioni, analizzando il relativo dataset, che contiene dati aggiuntivi come il numero di decessi, di ospedalizzati, di positivi e nuovi positivi, nonchè il numero di tamponi e di posti letto in terapia intensiva. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Regioni <a name=\"regioni\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analisi e Visualizzazioni <a name=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come nella sezione precedente, importiamo i dataset effettuiamo le prime analisi e visualizzazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caricamento dataset\n",
    "regione_orig = pd.read_csv('covid19_italy_region.csv',parse_dates=['Date'])\n",
    "popolazioni_orig=pd.read_csv('popolazioni.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regione = regione_orig.copy() \n",
    "popolazioni=popolazioni_orig.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dopo aver copiato i dataset di interesse, si esegue un 'merge' in base al nome della regione.\n",
    "Successivamente si visualizzano le dimensioni e le informazioni sul datset creato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regione2 = pd.merge(regione, popolazioni, left_on=\"RegionName\", right_on=\"Regione\").drop('Regione', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regione2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regione2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizziamo primi cinque elementi\n",
    "regione2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizziamo ultimi cinque elementi\n",
    "regione2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sono stati creati nuovi attributi per meglio analizzare la situazione delle diverse regioni sotto più punti di vista.\n",
    "Di seguito viene proposta una spiegazione di ogni nuova feature.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------\n",
    "LEGENDA\n",
    "\n",
    "Se a t(2) ho CurrentPositive=100 questo è uguale a NewPositive(t(0))+NewPositive(t(1))\n",
    "\n",
    "\"TotalHospitalizedPatient\" + \"HomeConfinement\" = \"CurrentPositive\"\n",
    "\n",
    "\"IntensiveCare\" + \"HospitalizedPatient\" = \"TotalHospitalizedPatients\"\n",
    "\n",
    "\"CurrentPositive\" + \"Deaths\" + \"Recovered\" = \"TotalPositiveCases\"\n",
    "\n",
    "Viene evidenziata una dipendenza tra i vari attributi.\n",
    "\n",
    "\n",
    "SPIEGAZIONE VARIABILI\n",
    "\n",
    "ratio_PIR_Pop: corrisponde al rapporto tra i posti letto in terapia intensiva a regime con tuatta la popolazione (entrambi i parametri sono relativi alla regione).\n",
    "\n",
    "ratio_IC_HP: corrisponde al rapporto tra i pazienti in terapia intensiva e quelli ospedalizzati (entrambi i parametri sono relativi alla regione).\n",
    "\n",
    "ratio_TPC_TeP: rappresenta il rapporto tra i casi positivi totali e il numero di test (tamponi) eseguiti sulla popolazione (entrambi i parametri sono relativi alla regione).\n",
    "\n",
    "ratio_TPC_Pop: definito anche come prevalenza, rappresenta il numero di casi positivi totali rispetto alla popolazione (entrambi i parametri sono relativi alla regione).\n",
    "\n",
    "ratio_D_Pop: definito anche come tasso di mortalità, corrisponde ai decessi sul totale della popolazione (entrambi i parametri sono relativi alla regione).\n",
    "\n",
    "ratio_TCP_Pop_gg: definito anche come tasso di incidenza grezzo, definisce il numero di casi in un dato intervallo temporale (entrambi i parametri sono relativi alla regione).\n",
    "\n",
    "proportion_IC_HP_TPC: proporzione tra tutti i pazienti in ospedale e il numero di casi positivi totali (entrambi i parametri sono relativi alla regione).\n",
    "\n",
    "proportion_HC_TPC: proporzione tra il numero di paziento ospedalizzati e il totale delle persone che hanno la patologia (entrambi i parametri sono relativi alla regione).\n",
    "\n",
    "proportion_CPC_TPC: proporzione tra totale dei pazienti in ospedale sommato a quelli confinati a casa e il numero di casi positivi totali (entrambi i parametri sono relativi alla regione).\n",
    "\n",
    "proportion_R_TPC: proporzione tra il numero di guariti e il numero di casi positivi totali (entrambi i parametri sono relativi alla regione).\n",
    "\n",
    "proportion_IC_PIR: definita anche come indice di occupazione, definisce il numero di posti letto occupati, in questo caso in terapia intensiva (entrambi i parametri sono relativi alla regione).\n",
    "\n",
    "proportion_D_TPC: definita anche come letalità, definisce il numero di decessi sul totale della popolazione (entrambi i parametri sono relativi alla regione). \n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per la creazione del tasso di incidenza è stato implementato il seguente codice in modo da avere per ogni regione e per ogni numero di giorni trascorso la rispettiva incidenza che rappresenta la frequenza di contagi.\n",
    "Es: il valore ottenuto dal rapporto verrà diviso per il numero di giorni trascorsi: al primo giorno avrò una divisione per 1, al secondo per 2, al 28 per 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tasso di Incidenza x 100000 abitanti\n",
    "regione2.insert(regione2.shape[-1], 'ratio_TCP_Pop_gg', (regione2['TotalPositiveCases']/regione2['Popolazione'])*100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regione2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione dell'incidenza relativa ad ogni regione per oggni giorno\n",
    "a=0\n",
    "b=int(regione2.shape[0]/21)\n",
    "i=1\n",
    "\n",
    "while b<regione2.shape[0]+1:  \n",
    "    i=1\n",
    "    for k in range(a,b):\n",
    "        rowIndex = regione2.index[k]\n",
    "        regione2.loc[rowIndex, 'ratio_TCP_Pop_gg'] = (regione2.loc[rowIndex, 'ratio_TCP_Pop_gg'])/i\n",
    "        i=i+1\n",
    "        \n",
    "    a=a+int(regione2.shape[0]/21)\n",
    "    b=b+int(regione2.shape[0]/21)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regione2.head(84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regione2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli indici creati vengono moltiplicati per i fattori 100, 10000 a seconda dei casi, come da prassi epidemiologica, al fine di ottenere valori in percentuale o rapportati ad un dato quantitativo di abitanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserisco nella tabella creata alcuni nuovi attributi di interesse e alcune misure di incidenza epidemiologica\n",
    "   \n",
    "#Valore di posti Intensivi x 10000\n",
    "regione2.insert(regione2.shape[-1], 'ratio_PIR_Pop', (regione2['PostiIntensivaRegime']/regione2['Popolazione'])*10000)\n",
    "#Valori percentuali\n",
    "regione2.insert(regione2.shape[-1],'ratio_IC_HP', (regione2['IntensiveCarePatients']/regione2['HospitalizedPatients'])*100)\n",
    "regione2.insert(regione2.shape[-1],'ratio_TPC_TeP', (regione2['TotalPositiveCases']/regione2['TestsPerformed'])*100)\n",
    "#Prevalenza x 10000 abitanti\n",
    "regione2.insert(regione2.shape[-1], 'ratio_TPC_Pop', (regione2['TotalPositiveCases']/regione2['Popolazione'])*10000)\n",
    "#Mortalità x 10000 abitanti\n",
    "regione2.insert(regione2.shape[-1], 'ratio_D_Pop', (regione2['Deaths']/regione2['Popolazione'])*10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valori percentuali\n",
    "regione2.insert(regione2.shape[-1],'proportion_IC_HP_TPC', ((regione2['HospitalizedPatients']+regione2['IntensiveCarePatients'])/regione2['TotalPositiveCases'])*100)\n",
    "regione2.insert(regione2.shape[-1],'proportion_HC_TPC', (regione2['HomeConfinement']/regione2['TotalPositiveCases'])*100)\n",
    "regione2.insert(regione2.shape[-1],'proportion_CPC_TPC', (regione2['CurrentPositiveCases']/regione2['TotalPositiveCases'])*100)\n",
    "regione2.insert(regione2.shape[-1],'proportion_R_TPC', (regione2['Recovered']/regione2['TotalPositiveCases'])*100)\n",
    "# %Occupazione\n",
    "regione2.insert(regione2.shape[-1],'proportion_IC_PIR', (regione2['IntensiveCarePatients']/regione2['PostiIntensivaRegime'])*100)\n",
    "# %Letalità \n",
    "regione2.insert(regione2.shape[-1],'proportion_D_TPC', (regione2['Deaths']/regione2['TotalPositiveCases'])*100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visulaizziamo Le informazioni relative al dataset creato, per valutare se vi sono valori NaN\n",
    "regione2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si osserva la presenza di valori nulli; questi verranno considerati in seguito. \n",
    "\n",
    "Per ora si considerano i dati relativi all'ultima data nota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleziono gli elementi degli attributi relativi all'ultimo giorno, dato che ogni elemento è cumulativo \n",
    "regione_new = regione2[regione2[\"Date\"] == max(regione2[\"Date\"])].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(regione2[\"Date\"])\n",
    "regione_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si prosegue con la creazione di un dataframe con i soli attributi di interesse relativi all'ultima data e per ogni regione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raggruppo per regione e seleziono gli attributi di interesse\n",
    "regione_new2=regione_new.groupby('RegionName')['HospitalizedPatients','CurrentPositiveCases', 'Deaths', 'IntensiveCarePatients', 'Recovered', \n",
    "                                               'HomeConfinement', 'TotalPositiveCases', 'TestsPerformed', \n",
    "                                               'Popolazione', 'PostiIntensivaRegime','ratio_PIR_Pop', 'ratio_IC_HP',\n",
    "                                               'ratio_TPC_TeP','ratio_TPC_Pop','ratio_D_Pop','ratio_TCP_Pop_gg',\n",
    "                                               'proportion_IC_HP_TPC','proportion_HC_TPC','proportion_CPC_TPC',\n",
    "                                               'proportion_R_TPC','proportion_IC_PIR','proportion_D_TPC'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stampo a video il dataframe\n",
    "regione_new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Stampo a video i dati di ogni regione, sfruttando una funzione di Pandas per evidenziare i dati sulla base della \n",
    "#loro rilevanza all'interno della colonna\n",
    "regione_new2.style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tabella sopra proposta rappresenta un utile strumento per valutare l'incidenza della malattia da diversi punti di vista.\n",
    "Come si osserva la Lombardia è la regione che possiede i valori più alti per quasi tutti gli indici.\n",
    "Considerando gli indici ordinari, cioè quelli che non sono stati derivati (quelli epidemiologici), si osserva che le regioni più interessate sono Lombardia, Emilia Romagna, Veneto e Piemonte.\n",
    "Tuttavia questi indici tengono in considerazione un solo aspetto e sono piuttosto rigidi.\n",
    "Andiamo ad osservare cosa emerge dagli indici derivati:\n",
    "\n",
    "ratio_PIR_Pop --> Emilia Romagna e Valle d'Aosta primeggiano per il numero di posti letto in terapia intensiva per 10000, decisamente inferiore rispetto a quella della Lombardia, nonostante abbiano una popolazione inferiore a quest'ultima. La Lombardia si attesta ad un livello simile a quello delle regioni centro-meridionali. \n",
    "\n",
    "ratio_IC_HP\t--> in questo caso le regioni che hanno un percentuale maggiore di pazienti in terapia intensiva, rispetto a quelli in ospedale, sono la Basilicata e la Campania. Quindi queste due regioni hanno più pazienti in terapia intensiva di quelli in ospedale nonostante la popolazione ed il numero di contagiati siano inferiori rispetto alla Lombardia, Veneto, Piemonte.\n",
    "\n",
    "ratio_TPC_TeP --> Lombardia, Marche e Valle d'Aosta emergono per aver il maggior numero di contagi rispetto al numero di tamponi effettuati. Tuttavia non è del tutto vero che all'aumentare del numero di tamponi effettuati in una regione, il numero di contagiati aumenta, in quanto il Veneto è al secondo posto come numero di tamponi eseguiti, ma di questi poco meno del 9% sono positivi.\n",
    "\n",
    "ratio_TPC_Pop --> Valle d'Aosta, Lombardia, Marche e P.A. Trento hanno i valori più alti di contagi rispetto alla popolazione.\n",
    "Questo porterebbe a pensare che minore è la popolazione maggiore sia il numero di contagi, quindi la rapidità risulta maggiore; tuttavia maggiore è la popolazione, maggiore è il numero di soggetti suscettibili al contagio. Per valutare questo aspetto è necessario osservare il Tasso di Incidenza.\n",
    "\n",
    "ratio_D_Pop\t--> La Lombardia ha il valore di morti per Covid x 10000 abitanti più alto. Ovviamente per quanto detto sopra maggiore è il bacino di soggetti influenzabili, maggiore è la probabilità di avere un numero alto di contagiati e quindi un alto numero di morti.\n",
    "\n",
    "ratio_TCP_Pop_gg --> il Tasso di Incidenza evidenzia che Valle d'Aosta e Lombardia sono le più interessate e quindi le più colpite dal punto di vista della frequenza dei contagi. Questo indice mette in evidenza che, sulla base di quanto accennato sopra, sia la rapidità che il bacino di influenza sono aspetti egualmente importanti nella diffusione di una malattia.\n",
    "Utile per realizzare strategie di controllo e risposta per la malattia.\n",
    "\n",
    "proportion_IC_HP_TPC --> Piemonte, Lazio, Liguria, Molise e Abruzzo sono le più interessate dal punto di vista dei pazienti che richiedono trattamenti ospedalieri per la malattia, rispetto al numero di contagiati.\n",
    "Questo fa pensare che qualora il numero di contagiati non sia eccessivo alcune regioni applichino di più la politica di cura ospedaliera, salvo il fatto dei trattamenti di terapia intensiva che di per sè sono necessari, compatibilmente alla capacità del loro sistema sanitario.\n",
    "\n",
    "proportion_HC_TPC --> Valle d'Aosta, Sardegna e Basilicata primeggiano per il numero di paziento positivi confinati a casa, ma si osserva che la maggior parte delle regioni ha valori elevati per questo indice. Comparando i valori con quelli dell'indice precedente emerge il fatto che le regioni con valori inferiori, relativi all'indice in esame, sono quelle che hanno un maggior numero di pazienti che necessitano di cure ospedaliere.\n",
    "\n",
    "proportion_CPC_TPC --> La Lombardia ha la percentuale minore di pazienti confinati a casa e ospedalizzati rispetto alla totalità dei casi positivi. Ciò significa che la restante percentuale, più alta rispetto alle altre regioni è composta da soggetti guariti e deceduti.\n",
    "\n",
    "proportion_R_TPC --> La componente percentuale complementare a quella del punto precedente risulta, nel caso della Lombardia, composta per lo più dai soggetti guariti.\n",
    "Nei casi più evidenti quali Emilia Romagna e Molise, si osserva che tale componente percentuale è per la maggior parte composta da soggetti deceduti.\n",
    "\n",
    "proportion_IC_PIR --> Lombardia, Marche e Valle d'Aosta hanno la più alta saturazione dei posti di terapia intensiva rispetto al numero di contagiati. Questo parametro è di sicuro molto importante per valutare l'impatto della malattia sulle disponibilità delle risorse intensive di ogni regione. Quindi non solo le regioni più popolose hanno criticità nella terapia intensiva, ma anche quelle con un numero di abitanti inferiori. Potrebbe essere uno spunto per valutare politiche di riadeguamento regionale delle risorse in caso di situazioni critiche come quella attuale.\n",
    "\n",
    "proportion_D_TPC --> La Lombardia, Emilia Romagna e Liguria hanno alti valori, tuttavia vale quanto riportato in 'proportion_R_TPC'\n",
    "\n",
    "Sulla base di quanto osservato emerge che i parametri di maggior valore sono proportion_IC_PIR e ratio_TCP_Pop_gg, pertanto le regioni maggiormente interessate dalla malattia sono Lombardia e Valle d'Aosta. Tuttavia gli altri indici restano validi per comprendere le varie sfaccettature del problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione di un nuovo dataframe per osservare i dati globali dell'Italia, relativamente alle features ordinarie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raggruppo per regione e seleziono gli attributi del dataset originale per ottenere una valutazione sull'italia\n",
    "regione_new3=regione_new.groupby('Date')['HospitalizedPatients','CurrentPositiveCases', 'Deaths', 'IntensiveCarePatients', 'Recovered', \n",
    "                                         'HomeConfinement', 'TotalPositiveCases', 'TestsPerformed',\n",
    "                                         'Popolazione', 'PostiIntensivaRegime'].sum()\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stampo i dati totali di tutte le regioni per avere una visione globale\n",
    "regione_new3.style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Realizzazione di un grafico delle dispersioni per ogni attributo\n",
    "scatterplot_matrix  = pd.plotting.scatter_matrix(regione_new2, alpha=0.2, figsize=(15, 15), diagonal='kde', s=200)\n",
    "for ax in scatterplot_matrix.flatten():\n",
    "    ax.xaxis.label.set_rotation(90)\n",
    "    ax.yaxis.label.set_rotation(0)\n",
    "    ax.yaxis.label.set_ha('right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo scatterplot non permette un immediata valutazione delle relazioni tra le features derivate.\n",
    "Si intuisce un rapporto di dipendenza per quanto riguarda quelle ordinarie.\n",
    "\n",
    "Si esegue un grafico heatmap per valutare meglio se è presente una correlazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dato che il grafico delle dispersioni è poco utile a intuire eventuali relazioni si effettua un matrice di correlazioni\n",
    "sns.set(font_scale=1.4)\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.heatmap(regione_new2.corr(),annot=True)\n",
    "plt.axes().set_title('Heatmap n°1',fontsize =20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalla matrice di correlazione si evidenzia il fatto che è presente un'effettiva dipendenza tra gli attributi appartenenti al dataset originale, uniti anche ai due attributi definiti come 'Popolazione' e 'PostiIntensivaRegime'. In questo contesto la correlazione è diretta. \n",
    "Per le misure di incidenza si evidenziano casi in cui si ha una correlazione diretta, ma anche casi in cui tale correlazione è indiretta, ciò è dovuto alla natura dell'operazione matematica di rapporto e proporzione. A tal proposito si nota la presenza anche di bassi valori di correlazione compresi tra circa -0.4 e 0.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si procede alla creazione di variabili per poter eseguire una visualizzazione grafica di alcune features del dataset e per meglio comprendere gli andamenti di queste, come riportato nella tabella 'background'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esecuzione di un sort dei dati all'interno di ogni attributo\n",
    "pop= popolazioni.sort_values(by= ['Popolazione'], ascending=False)\n",
    "positive_cases = regione_new.sort_values(by = ['CurrentPositiveCases'], ascending=False)\n",
    "intensive_cases= regione_new.sort_values(by = ['IntensiveCarePatients'], ascending=False)\n",
    "recovered= regione_new.sort_values(by = ['Recovered'], ascending=False)\n",
    "home_confinement = regione_new.sort_values(by = ['HomeConfinement'], ascending=False)\n",
    "deaths= regione_new.sort_values(by = ['Deaths'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settaggio paerametri per il plot\n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xticks(rotation = 90)\n",
    "sns.barplot(pop.Regione, pop.Popolazione, color='b')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xticks(rotation = 90)\n",
    "sns.barplot(positive_cases.RegionName, positive_cases.CurrentPositiveCases, color='b')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xticks(rotation = 90)\n",
    "sns.barplot(intensive_cases.RegionName, intensive_cases.IntensiveCarePatients, color='b')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xticks(rotation = 90)\n",
    "sns.barplot(recovered.RegionName, recovered.Recovered, color='b')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xticks(rotation = 90)\n",
    "sns.barplot(home_confinement.RegionName, home_confinement.HomeConfinement, color='b')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xticks(rotation = 90)\n",
    "sns.barplot(deaths.RegionName, deaths.Deaths, color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I grafici eseguiti sulle variabili ordinarie confermano quanto osservato nella tabella precedente di 'background'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A supporto delle osservazioni proposte per le features derivate, si esegue la rappresentazione grafica dei valori di queste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risultati in %\n",
    "regione_new2['proportion_IC_HP_TPC'].sort_values().plot.barh(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risultati in %\n",
    "regione_new2['proportion_HC_TPC'].sort_values().plot.barh(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risultati in %\n",
    "regione_new2['proportion_CPC_TPC'].sort_values().plot.barh(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risultati in %\n",
    "regione_new2['proportion_R_TPC'].sort_values().plot.barh(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risultati di Letalità in %\n",
    "regione_new2['proportion_D_TPC'].sort_values().plot.barh(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risultati in %\n",
    "regione_new2['ratio_IC_HP'].sort_values().plot.barh(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Risultati in %\n",
    "regione_new2['ratio_TPC_TeP'].sort_values().plot.barh(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Risultati di Prevalenza x 10000 abitanti\n",
    "regione_new2['ratio_TPC_Pop'].sort_values().plot.barh(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risultati dei Posti di Terapia Intensiva x 10000\n",
    "regione_new2['ratio_PIR_Pop'].sort_values().plot.barh(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risultati di Occupazione in %\n",
    "regione_new2['proportion_IC_PIR'].sort_values().plot.barh(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le raffigurazioni sugli indici evidenziano sì, la prevalenza della Lombardia nelle prime posizioni, ma mostrano come a seconda dell'indice considerato possono variare le regioni nelle successive posizioni. \n",
    "Sembra che le regioni del Nord siano le più interessate, concordemente al fatto che l'epidemia si è sviluppata proprio in quelle zone. Tuttavia appare non scontato il fatto che la regione con più contagi sia quella effettivamente più colpita; si veda infatti il numero di posti in terapia intensiva rispetto ai pazienti effettivamente bisognosi di queste cure: la Valle d'Aosta e le Marche risultano analogamente interessata come la Lombardia, nonostante il loro numero di contagi sia decisamente inferiore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito vengono valutati gli andamenti di alcune features per regione. Si rimanda alla sezione relativa alle Time Series per approfondimenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valutazione dell'anadamento dei pazienti ospedalizzati nell'arco di tempo presente nel dataset\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "regione.groupby(['Date','RegionName'])['HospitalizedPatients'].max().unstack().plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "regione.groupby(['Date','RegionName'])['IntensiveCarePatients'].max().unstack().plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "regione.groupby(['Date','RegionName'])['TestsPerformed'].max().unstack().plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il comportamento nel tempo degli attributi considerati evidenzia, in due casi, un andamento simil-esponenziale per le regioni, in modo particolare per la Lombardia e il Veneto(grafico n°3).\n",
    "In un caso (grafico n°2) l'andamento sembra più proporzionale.\n",
    "In tutte le raffigurazioni grafiche la Lombardia si presenta come la regione più interessata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora verranno considerati i valori NaN trovati all'inizio della trattazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regione2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(regione2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I dati Nan sono ottenuti da una divisione per il valore zero. La sostituzione porta all'inserimento del valore 0 \n",
    "regione3=regione2.fillna(0)\n",
    "#datset regione3 è quello completo\n",
    "regione3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regione3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a valutare se sono presenti valori 'inf' e dove sono; qualora ci fossero questi verranno sostituiti con 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attr1=regione3.iloc[:, 7:]\n",
    "infi=np.isinf(df_attr1)\n",
    "np.where(np.isinf(df_attr1))\n",
    "df_attr1.index[np.isinf(df_attr1).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attr1.columns.to_series()[np.isinf(df_attr1).any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regione3.iloc[47,21])\n",
    "print(regione3.iloc[48,21])\n",
    "print(regione3.iloc[367,22])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dato l'esiguo numero di valori infiniti si procede con una sostituzione manuale.\n",
    "Si procede poi con un controllo sull'effettiva sostituzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regione3.iloc[47,21] = 0.0\n",
    "regione3.iloc[48,21] = 0.0\n",
    "regione3.iloc[367,22] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attr1=regione3.iloc[:, 7:]\n",
    "infi=np.isinf(df_attr1)\n",
    "np.where(np.isinf(df_attr1))\n",
    "df_attr1.index[np.isinf(df_attr1).any(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si valuta se la correlazione tra le fatures, valutata nell'heatmap precedente solo sull'ultimo record cumulativo, è confermata per tutti i record del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.4)\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.heatmap(regione3.iloc[:,7:].corr(),annot=True)\n",
    "plt.axes().set_title('Heatmap n°2',fontsize =20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appare evidente la correlazione tra le features ordinarie e si evidenziano alcune correlazioni significative anche tra le features derivate.\n",
    "Es: 'proportion_IC_PIR' e 'ratio_TCP_Pop_gg' = 86%\n",
    "\n",
    "Osservando la sotto-mappa relativa alle features derivate si osserva che la correlazione più alta è proprio tra i due attributi citati nell'esempio qui sopra. Si tralasciano nella sotto-mappa in esame gli attributi con il 98%, in quanto strettamente dipendenti per la relazione matematica.\n",
    "Da un'analisi retrospettiva emerge quindi che due indici importanti sono appunto 'proportion_IC_PIR' e 'ratio_TCP_Pop_gg', che possono essere impiegati per valutare le regioni più colpite dalla malattia. Appare quindi confermato quanto esposto in precedenza nella tabella di background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PCA <a name=\"pca\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si prosegue ora con l'esecuzione dell'analisi PCA, prima sulle features ordinarie e poi su quelle derivate.\n",
    "Lo split dell'analisi in due parti serve per evitare una eccessiva richiesta di componenti principali per coprire la varianza totale.\n",
    "La PCA con features ordinarie viene eseguita senza gli attributi 'Popolazione' e 'PostiIntensivaRegfime', perchè sono elementi costanti per ogni regione e andrebbero a fornire un identificativo uguale, per finalità, alla label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attr=regione3.iloc[:, 7:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene creata una variabile relativa a quelle che sono definibili delle label per ogni regione, cioè i codici regionali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg=regione3.iloc[:, 3:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dati vengono scalati con un MaxMin scaler, dato che dalla letteratura si evidenzia una robustezza dell'algoritmo nello scaling in un dato range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=[0, 1])\n",
    "data_rescaled = scaler.fit_transform(df_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si esegue la PCA al fine di valutare il numero di componenti necessarie per coprire la Varianza totale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting dell'algoritmo di PCA con il dataset\n",
    "pca = PCA().fit(data_rescaled)\n",
    "#Plotting della somma cumulativa della Explained Variance\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Dataset Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dal grafico emerge che considerando 5 componenti si riesce a coprire tutta la varianza del dataset (circa 100%).\n",
    "Al fine di eseguire anche un'analisi grafica si considerano 3 componenti principali.\n",
    "Vengono eseguiti quindi i grafici 2D e 3D per la PCA, coprendo una Varianza totale > 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3, svd_solver='full')\n",
    "principalComponents = pca.fit_transform(data_rescaled)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2',\n",
    "                                                                  'principal component 3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vengono concatenate le label alle componenti principali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, df_reg], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2D PCA visulaization\n",
    "fig = plt.figure(figsize = (20,20))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "#ax = plt.axes(projection='3d') \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 20)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 20)\n",
    "#ax.set_zlabel('Principal Component 3', fontsize = 20)\n",
    "ax.set_title('2 component PCA', fontsize = 25)\n",
    "targets = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "colors = ['red', 'green', 'blue', 'yellow','black', 'c', 'crimson','chocolate', 'maroon', 'purple', 'fuchsia', 'lime', 'olive', \n",
    "          'navy', 'teal', 'aqua','burlywood','chartreuse','orange', 'pink']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['RegionCode'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 20)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D PCA visualization\n",
    "fig = plt.figure(figsize = (20,20))\n",
    "ax = plt.axes(projection='3d') \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 20)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 20)\n",
    "ax.set_zlabel('Principal Component 3', fontsize = 20)\n",
    "ax.set_title('3 component PCA', fontsize = 25)\n",
    "targets = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "colors = ['red', 'green', 'blue', 'yellow','black', 'c', 'crimson','chocolate', 'maroon', 'purple', 'fuchsia', 'lime', 'olive', \n",
    "          'navy', 'teal', 'aqua','burlywood','chartreuse','orange', 'pink']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['RegionCode'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 3']\n",
    "               , c = color\n",
    "               , s = 30)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dai grafici sembra che l'utilizzo di 2 e 3 componenti riesca a descrivere le classi relative alle regioni e a separarle adeguatamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene creato un dataframe contenente le componenti e le features esaminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeDf = pd.concat([principalDf, df_attr], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeDf = pd.concat([completeDf, df_reg], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione del dataframe con le correlazioni tra fetaures e componenti principali e successiva rappresentazione grafica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = completeDf.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.imshow(completeDf.iloc[:, :-1].corr(), cmap = plt.cm.YlOrRd, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = [i for i in range(len(completeDf.iloc[:, :-1].columns))]\n",
    "plt.xticks(tick_marks, completeDf.iloc[:, :-1].columns, rotation='vertical')\n",
    "plt.yticks(tick_marks, completeDf.iloc[:, :-1].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo caso le features più correlate sono (escludendo le 'components', osservando l'asse orizzontale):\n",
    "    \n",
    "PC1 --> features più correlate 1, 2, 3, 5 e 9\n",
    "\n",
    "PC2 --> features più correlate 7, 8 e 10\n",
    "\n",
    "PC3 --> features più correlate 6, 7 e 8\n",
    "\n",
    "Si confermano le correlazioni delle features evidenziate nell'heatmap n°1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con il successivo comando visualizziamo in dettaglio le componenti della pca per meglio capire quali features sono rilevanti all'interno di ogni componente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abs( pca.components_ ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pca.components_ ha la forma [n_components, n_features]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC1 --> features più importanti 2,3,5 e 10\n",
    "\n",
    "PC2 --> features più importanti 7 e 10\n",
    "\n",
    "PC3 --> features più importanti 6,7 e 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I passaggi eseguiti in precedenza per la PCA vengono riproposti, analizzando le sole features derivate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attr1=regione3.iloc[:, 19:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = MinMaxScaler(feature_range=[0, 1])\n",
    "data_rescaled1 = scaler1.fit_transform(df_attr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting dell'algoritmo di PCA con il dataset\n",
    "pca1 = PCA().fit(data_rescaled1)\n",
    "#Plotting della somma cumulativa della Explained Variance\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.cumsum(pca1.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Dataset Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1 = PCA(n_components=6)\n",
    "principalComponents1 = pca1.fit_transform(data_rescaled1)\n",
    "principalDf1 = pd.DataFrame(data = principalComponents1, columns = ['principal component 1', 'principal component 2', \n",
    "                                                                    'principal component 3', 'principal component 4', \n",
    "                                                                    'principal component 5','principal component 6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf1 = pd.concat([principalDf1, df_reg], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D PCA visualization\n",
    "fig = plt.figure(figsize = (20,20))\n",
    "ax = plt.axes(projection='3d') \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 20)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 20)\n",
    "ax.set_zlabel('Principal Component 3', fontsize = 20)\n",
    "ax.set_title('3 component PCA', fontsize = 25)\n",
    "targets = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "colors = ['red', 'green', 'blue', 'yellow','black', 'c', 'crimson','chocolate', 'maroon', 'purple', 'fuchsia', 'lime', 'olive', \n",
    "          'navy', 'teal', 'aqua','burlywood','chartreuse','orange', 'pink']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf1['RegionCode'] == target\n",
    "    ax.scatter(finalDf1.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf1.loc[indicesToKeep, 'principal component 2']\n",
    "               , finalDf1.loc[indicesToKeep, 'principal component 3']\n",
    "               , c = color\n",
    "               , s = 30)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeDf1 = pd.concat([principalDf1, df_attr1], axis = 1)\n",
    "completeDf1 = pd.concat([completeDf1, df_reg], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix1 = completeDf1.corr()\n",
    "corr_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.imshow(completeDf1.iloc[:, :-1].corr(), cmap = plt.cm.YlOrRd, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = [i for i in range(len(completeDf1.iloc[:, :-1].columns))]\n",
    "plt.xticks(tick_marks, completeDf1.iloc[:, :-1].columns, rotation='vertical')\n",
    "plt.yticks(tick_marks, completeDf1.iloc[:, :-1].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analisi PCA sulle features derivate mette in evidenza il fatto che le features ordinarie sembrano più adatte ad ottenere PC meglio correlate. \n",
    "Si evidenziano conferme alle correlazioni tra le varie features derivate, riscontrate nell'heatmap n°2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si traccia l'andamento dei Test per l'Italia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=regione2.groupby(\"Date\")[['TotalPositiveCases', 'Deaths', 'Recovered','TestsPerformed','HospitalizedPatients',\n",
    "                          'TotalHospitalizedPatients']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df(data, x, y, title=\"\", xlabel='Date', ylabel='Value', dpi=100):\n",
    "    plt.figure(figsize=(16,5), dpi=dpi)\n",
    "    plt.plot(x, y, color='tab:red')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.show()\n",
    "\n",
    "plot_df(data, x=data.Date, y=data.TestsPerformed, title='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dati sono stati raggruppati per regione; la funzione ordina per data e sposta i dati in ciascun gruppo. Infine, si applica una concatenazione per ottenere il dataframe nel suo formato originale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_group=regione3.groupby([\"RegionName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_by_group(key, value_df):\n",
    "    df = value_df.assign(group = key) \n",
    "    return (df.sort_values(by=[\"Date\"], ascending=True)\n",
    "        .set_index([\"Date\"])\n",
    "        .shift(1)\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflist = [lag_by_group(g, reg_group.get_group(g)) for g in reg_group.groups.keys()]\n",
    "pd.concat(dflist, axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## LAG <a name=\"lag\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I grafici seguenti sono relativi ad alcune features ordinarie, prima per l'Italia, poi per ogni regione, considerando 4 lag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_list=regione3['RegionName'].unique()\n",
    "#print(reg_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "plt.rcParams.update({'ytick.left' : False, 'axes.titlepad':10})\n",
    "\n",
    "\n",
    "#names = ['HospitalizedPatients', 'Deaths', 'IntensiveCarePatients', 'Recovered', 'HomeConfinement','TotalPositiveCases',\n",
    "      #   'TestsPerformed', 'ratio_PIR_Pop', 'ratio_IC_HP', 'ratio_TPC_TeP', 'ratio_TCP_Pop_gg', 'proportion_HC_TPC',\n",
    "       #  'proportion_CPC_TPC','proportion_R_TPC', 'proportion_IC_PIR','proportion_D_TPC']\n",
    "\n",
    "names2=['TotalPositiveCases', 'Deaths', 'Recovered','TestsPerformed','HospitalizedPatients', 'TotalHospitalizedPatients']\n",
    "\n",
    "#for r in reg_list:\n",
    " #   df_pannel = regione3.loc[regione3.RegionName==r, :]\n",
    "for n in names2:\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(11,5), sharex=True, sharey=True, dpi=100)\n",
    "    for i, ax in enumerate(axes.flatten()[:4]):\n",
    "        lag_plot(data[n], lag=i+1, ax=ax, c='firebrick')\n",
    "        ax.set_title('Lag ' + str(i+1))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le features selezionate mostrano che c'è correlazione, a livello nazionale, dato che non ho sparsità nelle occorrenze.\n",
    "\n",
    "Seguono i grafici dei lag relativi alle features per ogni regione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names = ['HospitalizedPatients', 'Deaths', 'IntensiveCarePatients', 'Recovered', 'HomeConfinement','TotalPositiveCases',\n",
    "         'TestsPerformed']\n",
    "\n",
    "for r in range(0, len(reg_list)):\n",
    "    df_pannel = regione3.loc[regione3.RegionName==reg_list[r], :]\n",
    "    print(reg_list[r])\n",
    "    for n in names:\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(11,5), sharex=True, sharey=True, dpi=100)\n",
    "        for i, ax in enumerate(axes.flatten()[:4]):\n",
    "            lag_plot(df_pannel[n], lag=i+1, ax=ax, c='firebrick')\n",
    "            ax.set_title('Lag ' + str(i+1))\n",
    "        fig.suptitle('{}'.format(n), y=1.05)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per le features originali si evidenzia una correlazione tra le osservazioni con e senza lag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names3 = ['ratio_IC_HP', 'ratio_TPC_TeP', 'ratio_TCP_Pop_gg', 'proportion_HC_TPC', \n",
    "          'proportion_CPC_TPC','proportion_R_TPC', 'proportion_IC_PIR','proportion_D_TPC']\n",
    "\n",
    "for r in range(0, len(reg_list)):\n",
    "    df_pannel = regione3.loc[regione3.RegionName==reg_list[r], :]\n",
    "    print(reg_list[r])\n",
    "    for n in names3:\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(11,5), sharex=True, sharey=True, dpi=100)\n",
    "        for i, ax in enumerate(axes.flatten()[:4]):\n",
    "            lag_plot(df_pannel[n], lag=i+1, ax=ax, c='firebrick')\n",
    "            ax.set_title('Lag ' + str(i+1))\n",
    "        fig.suptitle('{}'.format(n), y=1.05)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si evidenzia che è presente una maggior correlazione nelle osservazioni con lag per le features derivate relative a:  ratio_TCP_Pop_gg, proportion_IC_PIR, ratio_TPC_TeP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma del diagramma di lag fornisce indizi sulla struttura sottostante dei dati. Se è presente una forma lineare o si ha una forma tendente al lineare, questo suggerisce che probabilmente un modello di autoregressione è una scelta corretta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Autocorrelation e Partial Autocorrelation <a name=\"autocorrelation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito vengono riportati i grafici di autocorrelazione e autocorrelazione parziale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "names4 = ['HospitalizedPatients', 'IntensiveCarePatients', 'HomeConfinement','TotalPositiveCases', 'TestsPerformed']\n",
    "\n",
    "for r in range(0, len(reg_list)):\n",
    "    df_pannel = regione3.loc[regione3.RegionName==reg_list[r], :]\n",
    "    print(reg_list[r])\n",
    "    for n in names4:\n",
    "        fig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)\n",
    "        plot_acf(df_pannel[n].tolist(), lags=20, ax=axes[0])\n",
    "        plot_pacf(df_pannel[n].tolist(), lags=20, ax=axes[1])\n",
    "        fig.suptitle('{}'.format(n), y=1.05)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I grafici dei valori per l'ACF rientrano nell'intervallo di confidenza del 95% per lag> 2, rappresentato dalla banda azzurra, il che evidenzia la mancanza di autocorrelazione tra le osservazioni, di ogni feature analizzata, al di sopra di quel valore di lag. \n",
    "L'andamento di ogni feature sembra simile per ogni regione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names5 = ['ratio_IC_HP', 'ratio_TPC_TeP', 'ratio_TCP_Pop_gg', 'proportion_HC_TPC','proportion_CPC_TPC', 'proportion_IC_PIR']\n",
    "\n",
    "for r in range(0, len(reg_list)):\n",
    "    df_pannel = regione3.loc[regione3.RegionName==reg_list[r], :]\n",
    "    print(reg_list[r])\n",
    "    for n in names5:\n",
    "        fig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)\n",
    "        plot_acf(df_pannel[n].tolist(), lags=20, ax=axes[0])\n",
    "        plot_pacf(df_pannel[n].tolist(), lags=20, ax=axes[1])\n",
    "        fig.suptitle('{}'.format(n), y=1.05)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analoghe considerazioni vengono effettuate per le features derivate analizzate.\n",
    "Gli andamenti questa volta sono variabili per regione nel campo della stessa feature considerata.\n",
    "Anche in questo caso per lag>2 si osserva una mancanza di autocorrelazione tra le osservazioni delle features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dall'osservazione del diagramma dell'autocorrelazione parziale si può intuire quanti lag passati includere nell'equazione di previsione di un modello auto-regressivo (AR). Nella maggior parte dei casi per lag>1 si rientra nella banda di confidenza, quindi il modello AR sarà di tipo 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Time Series <a name=\"ts\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa sezione ci dedichiamo alle seguenti analisi relative alle time series:\n",
    "1. Visualizzazione trend \n",
    "2. Visualizzazione seasonality\n",
    "3. Test stazionarietà\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizziamo il dataframe completo regione3, aggiungendo alcune features che specificano il giorno della settimana e il numero di settimana dell'anno. Per questo lasciamo invariato il dataframe originale e lavoriamo su un dataset temporaneo chiamato ts_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df=regione3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggiungiamo le colonne day_of_week e week_of_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df['day_of_week'] = ts_df.Date.apply(lambda x: x.dayofweek)\n",
    "ts_df['week_of_year'] = ts_df.Date.apply(lambda x: x.weekofyear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stampiamo le prime cinque righe del dataframe appena modificato per vedere che le nuove colonne siano state effettivamente aggiunte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Visualizzazione Trend**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniziamo lo studio delle time series analizzandone i grafici e vedendo se esistono trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come primo passo definiamo due funzioni utili per disegnare i grafici relativi agli andamenti delle time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Plot\n",
    "def plot_df(df, x, y, title=\"\", xlabel='Date', ylabel='Value', dpi=100):\n",
    "    plt.figure(figsize=(16,5), dpi=dpi)\n",
    "    plt.plot(x, y, color='tab:red')\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disegna i grafici di tutte le regioni in relazione ad una singola feature.\n",
    "def plot_ts_regions(df, x_ax, y_ax, regionlist, frequency='dayly'):\n",
    "    for r in range(0, len(regionlist)):\n",
    "        temp1 = df.loc[df.RegionName==regionlist[r], :]\n",
    "        plot_df(temp1, x=temp1[x_ax], y=temp1[y_ax], title=frequency+' trend of '+y_ax+' '+reg_list[r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniziamo generando i grafici relativi alle principali features, per tutte le regioni. Daremo poi delle considerazioni finali sulle informazioni che possiamo dedurre dai grafici generati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New positive cases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'NewPositiveCases', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hospitalized Patients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'HospitalizedPatients', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deaths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'Deaths', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intensive Care patients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'IntensiveCarePatients', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recovered**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'Recovered', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Home confinement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'HomeConfinement', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total Positive cases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'TotalPositiveCases', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests performed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'TestsPerformed', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ratio IC_HP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'ratio_IC_HP', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ration TPC_TeP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'ratio_TPC_TeP', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ratio TCP_Pop_gg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'ratio_TCP_Pop_gg', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proportion HC_TPC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'proportion_HC_TPC', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proportion CPC_TPC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'proportion_CPC_TPC', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proportion R_TPC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'proportion_R_TPC', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proportion IC_PIR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'proportion_IC_PIR', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proportion D_TPC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts_regions(ts_df, 'Date', 'proportion_D_TPC', reg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Considerazioni finali sui trend**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NewPositiveCases: si nota un trend in crescendo nelle regioni più colpite, nelle regioni meno colpite invece si alternano picchi positivi e negativi, mantenendo comunque un trend crescente.\n",
    "\n",
    "* HospitalizedPatients: in questo caso il trend risulta crescente in tutte le regioni, con curve continue nelle regioni più colpite.\n",
    "\n",
    "* Deaths: in generale il trend è crescente nelle regioni più colpite, come la Lombardia, mentre in quelle meno colpite si hanno periodi spesso costanti, seguiti da alcuni picchi. Da notare che l'unica regione con valore costante a 0 è la Basilicata.\n",
    "\n",
    "* IntensiveCarePatients: trend crescente per tutte le regioni.\n",
    "\n",
    "* Recovered: trend crescente in tutte le regioni, con una maggiore ripidità nelle regioni meno colpite.\n",
    "\n",
    "* HomeConfinement: trend crescente in tutte le regioni.\n",
    "\n",
    "* TotalPositiveCases: trend crescente in tutte le regioni.\n",
    "\n",
    "* TestsPerformed: trend crescente in tutte le regioni. \n",
    "\n",
    "* ratio_IC_HP: non è identificabile un vero e proprio trend, solo la regione Veneto sembra avere un trend in decrescita.\n",
    "\n",
    "* ratio_TPC_TeP: crescente per tutte le regioni\n",
    "\n",
    "* ratio_TCP_Pop_gg: crescente per tutte le regioni\n",
    "\n",
    "* proportion_HC_TPC: in generale è crescente per quasi tutte le regioni, eccetto il Piemonte che ha un trend decrescente.\n",
    "\n",
    "* proportion_CPC_TPC: Decrescente per le regioni con il maggior numero di casi, come Lombardia, Piemonte, Veneto ed Emilia Romagna, mentre nelle restanti abbiamo trend stabili, quasi costanti.\n",
    "\n",
    "* proportion_R_TPC: abbiamo la Basilicata costante a 0, mentre per le altre regioni abbiamo un trend in generale decrescente per le regioni meno colpite e crescente invece per quelle più colpite.\n",
    "\n",
    "* proportion_IC_PIR: in generale crescente in tutte le regioni\n",
    "\n",
    "* proportion_D_TPC: in generale crescente in tutte le regioni, ad eccezione della Basilicata, costante a 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Visualizzazione Stagionalità\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come nella sezione relativa alla visualizzazione dei trend, anche qui definiamo delle funzioni utili alla creazione dei grafici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly_seasonality_plot(df, featureName, reg_list, weeks, colors, week_dict):\n",
    "    for r in range(0, len(reg_list)):\n",
    "        temp1 = df.loc[df.RegionName==reg_list[r], :]\n",
    "        print(reg_list[r])\n",
    "        plt.figure(figsize=(14,10), dpi= 100)\n",
    "        for i, y in enumerate(weeks):\n",
    "            if i >= 0:        \n",
    "                plt.plot('day_of_week', featureName, data=temp1.loc[temp1.week_of_year==y, :], color=colors[i], label=y)\n",
    "                plt.text(temp1.loc[temp1.week_of_year==y, :].shape[0]-.9, temp1.loc[temp1.week_of_year==y, featureName][-1:].values[0], week_dict[y], fontsize=12, color=colors[i])\n",
    "        plt.gca().set(title=featureName+' seasonality check', xlabel='day', ylabel=featureName)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = ts_df['week_of_year'].unique()\n",
    "# Prep Colors\n",
    "np.random.seed(100)\n",
    "mycolors = np.random.choice(list(mpl.colors.XKCD_COLORS.keys()), len(weeks), replace=False)\n",
    "week_dict={9:'1st week',10:'2nd week',11:'3rd week',12:'4th week'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche qui, come nella sezione precedente, generiamo prima tutti i grafici, e poi ne analizziamo i risultati in seguito nelle considerazioni finali sulla stagionalità."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Legenda**\n",
    "\n",
    "\n",
    "**Settimane**\n",
    "\n",
    "* 1st week->prima settimana dall'inizio del contagio in Italia\n",
    "* 2nd week->seconda settimana dall'inizio del contagio in Italia\n",
    "* 3rd week->terza settimana dall'inizio del contagio in Italia\n",
    "* 4th week->quarta settimana dall'inizio del contagio in Italia\n",
    "\n",
    "\n",
    "**Giorni**\n",
    "\n",
    "* 0: Lunedì\n",
    "* 1: Martedì\n",
    "* 2: Mercoledì\n",
    "* 3: Giovedì\n",
    "* 4: Venerdì\n",
    "* 5: Sabato\n",
    "* 6: Domenica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New Positive cases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df, 'NewPositiveCases', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hospitalized Patients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df, 'HospitalizedPatients', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deaths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df, 'Deaths', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intensive care patients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df, 'IntensiveCarePatients', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recovered**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df, 'Recovered', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Home confinement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df, 'HomeConfinement', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total positive cases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df, 'TotalPositiveCases', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests performed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df, 'TestsPerformed', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ratio IC_HP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df, 'ratio_IC_HP', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ratio TPC_TeP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df, 'ratio_TPC_TeP', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ratio TCP_Pop_gg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df, 'ratio_TCP_Pop_gg', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proportion HC_TPC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df, 'proportion_HC_TPC', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proportion CPC_TPC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df,'proportion_CPC_TPC', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proportion R_TPC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df, 'proportion_R_TPC', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proportion IC_PIR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df, 'proportion_IC_PIR', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proportion D_TPC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_seasonality_plot(ts_df, 'proportion_D_TPC', reg_list, weeks, mycolors, week_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Considerazioni finali sulla stagionalità**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per quanto riguarda la stagionalità, in generale è difficile affermare che sia presente in qualcuna delle features considerate, soprattutto per la mancanza di uno storico di dati ed il breve periodo che ricoprono i dati disponibili. Per provare a vedere se esiste una stagionalità, abbiamo considerato le settimane, provando a vedere se determinati giorni, ad esempio il sabato e la domenica, avessero andamenti comuni e accentuati, in positivo o in negativo. Per features ordinarie, che hanno trend esclusivamente crescenti, come ad esempio IntensiveCarePatients, si potrebbe notare un minimo di stagionalità, mentre per le feature derivate no. Sarebbe stato indicativo e utile trovare stagionalità nel numero di nuovi casi positivi, ad esempio se si fosse trovata una marcata stagionalità nei giorni del week-end, si poteva supporre che, siccome nel weekend le persone tendono ad uscire e a creare assembramenti, si ha un maggiore numero di nuovi positivi. Questo però non accade, difatti anche se avessimo trovato stagionalità, i tempi di incubazione del virus sono diversi per ogni persona, quindi incrementi stagionali del numero di positivi in un dato giorno della settimana potrebbero essere semplici coincidenze. Concludendo, per i dati attualmente disponibili secondo noi non ci sono prove a sufficienza per affermare che vi sia stagionalità."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Verifica stazionarietà\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def test_stationarity(timeseries):\n",
    "    \"\"\"\n",
    "    Check Stationariety of time series.\n",
    "    Please use np.array or pd.series as Input with your TS data only\n",
    "    \"\"\"\n",
    "    #Convert numpy array to pandas serie\n",
    "    if type(timeseries) is np.ndarray:\n",
    "        df_timeseries = pd.Series(timeseries) \n",
    "        \n",
    "    try:\n",
    "        #Determing rolling statistics\n",
    "        rolmean = df_timeseries.rolling(window=12).mean()\n",
    "        rolstd = df_timeseries.rolling(window=12).std()\n",
    "\n",
    "        #Plot rolling statistics:\n",
    "        orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "        mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "        std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "        plt.legend(loc='best')\n",
    "        plt.title('Rolling Mean & Standard Deviation')\n",
    "        plt.show(block=False)\n",
    "\n",
    "        #Perform Dickey-Fuller test:\n",
    "        print('Results of Dickey-Fuller Test:')\n",
    "\n",
    "        dftest = adfuller(timeseries, autolag='AIC')\n",
    "        dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "        for key,value in dftest[4].items():\n",
    "            dfoutput['Critical Value (%s)'%key] = value\n",
    "        \n",
    "        # print(dfoutput)\n",
    "    \n",
    "        return dftest, dfoutput\n",
    "    except Exception as message:\n",
    "        print(f\"Impossible to calc the stationariery of your TS: {message}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_stationary=ts_df.copy()\n",
    "temp_stationary=temp_stationary[temp_stationary.RegionName == 'Lombardia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_stationary['Date']=pd.to_datetime(temp_stationary['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = temp_stationary['Date'].values\n",
    "y1 = temp_stationary['NewPositiveCases'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest, dfoutput = test_stationarity(y1)\n",
    "dfoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_stationary.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = temp_stationary['Date'].values\n",
    "y1 = temp_stationary['IntensiveCarePatients'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest, dfoutput = test_stationarity(y1)\n",
    "dfoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Detrend\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our TS\n",
    "df_detrend = ts_df.copy()\n",
    "df_detrend=df_detrend[df_detrend.RegionName=='Lombardia']\n",
    "df_detrend.index = df_detrend.Date\n",
    "df_detrend = df_detrend.drop('Date',axis=1)\n",
    "plt.plot(df_detrend.index, df_detrend.IntensiveCarePatients)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended = signal.detrend(df_detrend.IntensiveCarePatients.values)\n",
    "plt.plot(df_detrend.index, detrended)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('IntensiveCarePatients by subtracting the least squares fit', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Clustering <a name=\"clustering\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa sezione verranno eseguite delle procedure di clustering per valutare, sulla base di coppie di features selezionate, se le regioni del nord hanno un comportamento omogeneo ed uniforme oppure se le altre regioni italiane hanno adottato comportamenti simili alle regioni in cui il virus si è sviluppato maggiormente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene creato un nuovo dataframe contenente le feature da utilizzare per la clusterizzazione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "regione_new.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rimuoviamo le feature che non utilizzeremo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_k=regione_new.drop(['index', 'SNo', 'Date', 'Country', 'RegionCode', 'RegionName','Latitude', 'Longitude',\n",
    "                         'ratio_PIR_Pop','ratio_IC_HP', 'ratio_TPC_TeP', 'ratio_TPC_Pop', 'ratio_D_Pop',\n",
    "                         'proportion_IC_HP_TPC', 'proportion_HC_TPC','proportion_R_TPC','proportion_D_TPC'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_k\n",
    "data_k.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considero test eseguiti e casi totali\n",
    "data_km=data_k.iloc[:,8:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo delle funzioni per il calcolo del wcss, ovvero per trovare il numero ottimale di cluster, e per la computazione e rappresentazione grafica del k-means e del dbscan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_wcss(df):\n",
    "    wcss=[]\n",
    "    for i in range(1,11):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10)\n",
    "        y_means = kmeans.fit(df)\n",
    "        wcss.append(y_means.inertia_)\n",
    "    #Plotting WCSS to find the number of clusters\n",
    "    plt.plot(range(1,11), wcss)\n",
    "    plt.xlabel(\"No. of clusters\")\n",
    "    plt.ylabel(\" Within Cluster Sum of Squares\")\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def kmeans_function(numberClusters, df, xlabel, ylabel ):\n",
    "    # Fitting K-Means \n",
    "    kmeans = KMeans(n_clusters = numberClusters, init = 'k-means++', random_state = 42)\n",
    "    y_kmeans = kmeans.fit_predict(df)\n",
    "    #data_km3 = df\n",
    "    #numerazione cluster partendo da 1\n",
    "    y_kmeans4=y_kmeans\n",
    "    y_kmeans4=y_kmeans+1\n",
    "    # nuovo dataframe\n",
    "    cluster = pd.DataFrame(y_kmeans4)\n",
    "    # Aggiungo i cluster\n",
    "    df['cluster'] = cluster\n",
    "    #Media dei clusters\n",
    "    kmeans_mean_cluster = pd.DataFrame(round(df.groupby('cluster').mean(),1))\n",
    "    kmeans_mean_cluster\n",
    "    #cluster dei positivi vs morti\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(df.iloc[:,0], df.iloc[:,1],c=y_kmeans, cmap='rainbow')  # plot points with cluster dependent colors\n",
    "    plt.title('Clustering')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    data_r= pd.DataFrame()\n",
    "    data_r['Region']=regione_new['RegionName']\n",
    "    data_r['Level']=y_kmeans4\n",
    "    for group in range(1,(numberClusters + 1)):\n",
    "        re=data_r.loc[data_r['Level']==group]\n",
    "        listofre= list(re['Region'])\n",
    "        print(\"Group\", group, \":\", listofre)\n",
    "    return y_kmeans4\n",
    "\n",
    "def dbscan_function(df, regione_new, eps, min_samples):\n",
    "    X = df\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    #cluster dei dati\n",
    "    dbscan = DBSCAN(eps=eps, min_samples = min_samples)\n",
    "    clusters2 = dbscan.fit_predict(X_scaled)\n",
    "    # plot dei cluster \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=clusters2, cmap=\"plasma\")\n",
    "    plt.xlabel(\"Feature 0\")\n",
    "    plt.ylabel(\"Feature 1\")\n",
    "    c2=pd.DataFrame(np.array(clusters2).T)\n",
    "    cl_name=pd.concat([c2, regione_new['RegionName']], axis = 1)\n",
    "    print('', cl_name)\n",
    "    return clusters2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## K-Means <a name=\"kmeans\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a rappresentare graficamente la funzione wcss, within cluster sum of squares, per selezionare l'opportuno numero di cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_wcss(data_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dal grafico della wcss si deduce che il numero ottimale di cluster è 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eseguiamo la procedura di clustering considerando il numero di casi confermati rispetto al numero di test eseguiti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans=kmeans_function(5, data_km, \"No. of confirmed cases\", \"Test Perfomed by the Region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dall'analisi dei cluster si evidenzia che le regioni del nord, che sono le più interessate dal contagio, perchè sviluppatosi in quelle zone, rientrano in cluster diversi, pertanto il comportamento di alcune di queste risulta simile a quello delle altre regioni italiane meno interessate. Si osservi ad esempio il gruppo numero 5, nel quale compare il Lazio ed il Piemonte, e il gruppo 1. Si evidenzia anche la presenza di cluster ad un singolo elemento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selezioniamo dal datafreme le seguenti feature: Frequenza dell'infezione per regione ed occupazione delle terapie intensive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_km1= pd.concat([data_k.iloc[:,12], data_k.iloc[:,14]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a rappresentare graficamente la funzione wcss, within cluster sum of squares, per selezionare l'opportuno numero di cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_wcss(data_km1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dal grafico della wcss si deduce che il numero opportuno di cluster è 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans1=kmeans_function(4, data_km1, \"Frequencies of Infection by the Region\", \"Occupation IC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche in questo contesto si riscontrano comportamenti simili nelle diverse regioni di Italia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selezioniamo come feature il numero di casi positivi per regione ed il numero di guariti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_km2= pd.concat([data_k.iloc[:,4], data_k.iloc[:,6]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a rappresentare graficamente la funzione wcss, within cluster sum of squares, per selezionare l'opportuno numero di cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_wcss(data_km2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dal grafico sembra che sul 4 vi è una ulteriore lieve flessione, quindi poniamo come numero di cluster tale valore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans2=kmeans_function(4, data_km2, \"Current Positive Cases by the Region\", \"Recovered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordemente a quanto riscontrato sopra, si nota la presenza di cluster eterogenei (composti sia da regioni del nord, del centro e del sud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si procede con la creazione del nuovo dataframe, selezionando come feature il numero di casi positivi per la regione ed il numero di decessi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_km3= pd.concat([data_k.iloc[:,4], data_k.iloc[:,7]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a rappresentare graficamente la funzione wcss, within cluster sum of squares, per selezionare l'opportuno numero di cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_wcss(data_km3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dal grafico sembra che il numero ottimale di clusters sia 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans3=kmeans_function(4, data_km3,\"Current Positive Cases by the Region\", \"Deaths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche i questo caso notiamo che vi sono cluster eterogenei, comprendenti sia regioni del nord, che centro e sud. In particolare si nota che la Lombardia distanzia di molto tutte le altre regioni, formando un cluster unico, mentre i rimanenti cluster si concentrano in una porzione più piccola, con casistiche più simili. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo un nuovo dataframe, considerando come features per la clusterizzazione il numero di casi positivi per regione e le frequenze di infezione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_km4= pd.concat([data_k.iloc[:,4], data_k.iloc[:,12]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a rappresentare graficamente la funzione wcss, within cluster sum of squares, per selezionare l'opportuno numero di cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_wcss(data_km4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche in questo caso il numero di cluster che sembra ottimale è 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans4=kmeans_function(4, data_km4, \"Current Positive Cases by the Region\", \"Frequencies of Infection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nota anche per questa clusterizzazione la presenza di gruppi di regioni eterogenee. Da notare inoltre che le regioni nei vari gruppi si distribuiscono lungo l'asse verticale, indicando quindi che anche regioni con numero di casi positivi simili hanno frequenze di infezioni diverse, quindi una situazione più o meno grave rispetto alle altre nello stesso gruppo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DBSCAN <a name=\"dbscan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo un'altra tipologia di clustering, per controllare l'effettiva efficacia dei gruppi formati con il kmeans. Utilizzeremo gli stessi dataframe utilizzati precedentemente, ovvero i dataframe basati su coppie di feature. L'analisi viene effettuata seguendo lo stesso ordine utilizzato nel K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "clusters=dbscan_function(data_km, regione_new, 0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters1=dbscan_function(data_km1, regione_new, 0.4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters2=dbscan_function(data_km2, regione_new, 0.3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters3=dbscan_function(data_km3, regione_new, 0.3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters4=dbscan_function(data_km4, regione_new, 0.3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misuriamo le prestazioni di clustering tramite Rand Index: questo indice misura la similarità tra i cluster prodotti da due algoritmi. Dato che con K-means abbiamo valutato il giusto numero di cluster da creare (quindi il numero di cluster è noto), l'indice sopra citato valuterà la prestazione di DBSCAN rispetto a K_means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "#Prestazioni DBSCAN rispetto a K-Means\n",
    "\n",
    "#Clustering caso 1\n",
    "print(\"ARI =\", adjusted_rand_score(y_kmeans, clusters))\n",
    "#Clustering caso 2\n",
    "print(\"ARI =\", adjusted_rand_score(y_kmeans1, clusters1))\n",
    "#Clustering caso 3\n",
    "print(\"ARI =\", adjusted_rand_score(y_kmeans2, clusters2))\n",
    "#Clustering caso 4\n",
    "print(\"ARI =\", adjusted_rand_score(y_kmeans3, clusters3))\n",
    "#Clustering caso 5\n",
    "print(\"ARI =\", adjusted_rand_score(y_kmeans4, clusters4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dai risultati ottenuti nell'ARI possiamo affermare che le due tipologie di clustering performano in 3 casi su 5 perfettamente allo stesso modo.\n",
    "Nel caso due si evidenzia una grande diversità, al contrario del caso 5 in cui la difformità è più ridotta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Regressione <a name=\"regression\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m=regione3.drop(['SNo', 'Date', 'Country','RegionCode', 'RegionName','Latitude', 'Longitude','TotalHospitalizedPatients','CurrentPositiveCases',\n",
    "                      'NewPositiveCases', 'Popolazione','TotalPositiveCases','ratio_PIR_Pop','ratio_IC_HP','TestsPerformed',\n",
    "                      'ratio_TPC_TeP', 'ratio_TPC_Pop', 'ratio_D_Pop','proportion_IC_HP_TPC', 'proportion_HC_TPC','proportion_R_TPC',\n",
    "                      'proportion_D_TPC','proportion_CPC_TPC'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor)\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler(feature_range=[0, 1])\n",
    "data_m_sc=scaler.fit_transform(data_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_name=['ratio_TCP_Pop_gg','proportion_IC_PIR']\n",
    "X_df = data_m.copy()\n",
    "X_df.drop(['ratio_TCP_Pop_gg','proportion_IC_PIR'], axis=1, inplace=True)\n",
    "y_df = data_m[features_name[0]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si considera ora il precedente dataframe creato e si procede alla realizzazione del set di train e di test.\n",
    "Si vogliono ottenere dei valori in output, corrispondenti di volta in volta alle varie features considerate nella variabile y, sulla base di determinate features scelte come input. \n",
    "La finalità è quella di avere un algoritmo che, dati dei valori in ingresso (es: totale contagiati, guariti, morti, ecc.) fornisca un valore il più possibile certo di frequenza di contagi e rapporto tra pazienti in terapia intensiva e posti disposnibili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df,train_size=0.7, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vengono create delle strutture per permettere l'analisi del dataframe con più regressori e memorizzare i valori di RMSE e R^2, per ognuno di essi.\n",
    "Le procedure descritte successivamente verranno ripetute per ciascun attributo 'ratio_TCP_Pop_gg', 'proportion_IC_PIR', da considerarsi come feature di output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model0 = []\n",
    "RMSE0 = [] #misura la differenza tra i valori\n",
    "R_sq0 = [] #indica la corretteza del modello usato\n",
    "cv0 = KFold(10, random_state = 42)\n",
    "\n",
    "#funzione che, a seconda dell'algoritmo impigato, calcola i parametri RMSE e R_sq.\n",
    "#Il nome dell'algoritmo viene inserito nella lista corrispondente, così come anche i parametri RMSE e R_sq.\n",
    "def input_scores0(name, model, x, y):\n",
    "    #accodo i nomi degli algoritmi\n",
    "    Model0.append(name)\n",
    "    #ricavo il valore meVengono create delle strutture per permettere l'analisi del dataframe con più regressori e memorizzare i valori di RMSE e R^2, per ognuno di essi.\n",
    "    #eseguo poi la radice quadrata di tale valore medio per avere RMSE\n",
    "    cv_result0=cross_val_score(model, x, y, cv=cv0, scoring='neg_mean_squared_error').mean()\n",
    "    RMSE0.append(np.sqrt((-1) * cv_result0))\n",
    "    #ricavo il valore medio del risultato prodotto dalla funzione di cross validatio (più valori del Coefficiente di determinazione)\n",
    "    R_sq0.append(cross_val_score(model, x, y, cv=cv0, scoring='r2').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito vengono inseriti i regressori esaminati, considerando in y, l'attributo 'ratio_TCP_Pop_gg'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names0 = ['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'K Neighbors Regressor', 'Decision Tree Regressor', \n",
    "         'Random Forest Regressor', 'Gradient Boosting Regressor', 'Adaboost Regressor','Suppport Vector Machine']\n",
    "models0 = [LinearRegression(), Ridge(), Lasso(), KNeighborsRegressor(), DecisionTreeRegressor(), RandomForestRegressor(),\n",
    "          GradientBoostingRegressor(), AdaBoostRegressor(), SVR()]\n",
    "\n",
    "for name0, model0 in zip(names0, models0):\n",
    "    input_scores0(name0, model0, X_train, y_train)\n",
    "    \n",
    "#Costruisco un dataframe partendo da un dizionario\n",
    "evaluation0 = pd.DataFrame({'Model0': Model0, 'RMSE0': RMSE0, 'R Squared0': R_sq0})\n",
    "print(\"Valutazione del punteggio di training per il Dataset d'origine scalato: \")\n",
    "evaluation0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalla tabella è possibile osservare che l'algoritmo più performante, usando una configurazione di default, è il Gradient boosting regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene creato un nuovo set di training e test per eseguire l'analisi sul secondo attributo considerato come output, cioè proportion_IC_PIR, con conseguente inserimento dei regressori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df1 = data_m[features_name[1]].copy()\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_df, y_df1,train_size=0.7, random_state=42, shuffle=True)\n",
    "\n",
    "Model1 = []\n",
    "RMSE1 = [] #misura la differenza tra i valori\n",
    "R_sq1 = [] #indica la corretteza del modello usato\n",
    "cv1 = KFold(10, random_state = 42)\n",
    "\n",
    "#funzione che, a seconda dell'algoritmo impigato, calcola i parametri RMSE e R_sq.\n",
    "#Il nome dell'algoritmo viene inserito nella lista corrispondente, così come anche i parametri RMSE e R_sq.\n",
    "def input_scores1(name, model, x, y):\n",
    "    #accodo i nomi degli algoritmi\n",
    "    Model1.append(name)\n",
    "    #ricavo il valore medio del risultato ottenuto dalla funzione di cross validation (ho più valori medi negativi che poi moltiplico per -1)\n",
    "    #eseguo poi la radice quadrata di tale valore medio per avere RMSE\n",
    "    cv_result1=cross_val_score(model, x, y, cv=cv1, scoring='neg_mean_squared_error').mean()\n",
    "    RMSE1.append(np.sqrt((-1) * cv_result1))\n",
    "    #ricavo il valore medio del risultato prodotto dalla funzione di cross validatio (più valori del Coefficiente di determinazione)\n",
    "    R_sq1.append(cross_val_score(model, x, y, cv=cv1, scoring='r2').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names1 = ['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'K Neighbors Regressor', 'Decision Tree Regressor', \n",
    "         'Random Forest Regressor', 'Gradient Boosting Regressor', 'Adaboost Regressor','Suppport Vector Machine']\n",
    "models1 = [LinearRegression(), Ridge(), Lasso(), KNeighborsRegressor(), DecisionTreeRegressor(), RandomForestRegressor(),\n",
    "          GradientBoostingRegressor(), AdaBoostRegressor(), SVR()]\n",
    "\n",
    "for name1, model1 in zip(names1, models1):\n",
    "    input_scores1(name1, model1, X_train1, y_train1)\n",
    "    \n",
    "#Costruisco un dataframe partendo da un dizionario\n",
    "evaluation1 = pd.DataFrame({'Model1': Model1, 'RMSE1': RMSE1, 'R Squared1': R_sq1})\n",
    "print(\"Valutazione del punteggio di training per il Dataset d'origine scalato: \")\n",
    "evaluation1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche nel secondo caso appare evidente che l'algoritmo migliore è il Gradient boosting regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si considera ora l'ottimizzazione dei parametri tramite procedura di grid search, per entrambi i casi sopra considerati. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applicazione GridSearch su Gradient Boosting\n",
    "#seleziono i parametri da stimare\n",
    "param_grid = {'n_estimators':[10, 50, 100],\n",
    "              'max_depth':[5, 10, 15],  \n",
    "              'min_samples_split':[10, 15, 20], \n",
    "              'learning_rate':[0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "#imposto l'algoritmo di ricerca\n",
    "clf = GridSearchCV(GradientBoostingRegressor(random_state=10), \n",
    "                   param_grid = param_grid, scoring='r2', \n",
    "                   cv=cv0).fit(X_train, y_train)\n",
    "\n",
    "#stampo i parametri migliori dell'algoritmo\n",
    "print(clf.best_estimator_) \n",
    "#stampo il mioglior valore per il parametro voluto\n",
    "print(\"R Squared:\",clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applicazione GridSearch su Gradient Boosting\n",
    "#seleziono i parametri da stimare\n",
    "param_grid1 = {'n_estimators':[10, 50, 600],\n",
    "              'max_depth':[5, 10, 15],  \n",
    "              'min_samples_split':[10, 15, 70], \n",
    "              'learning_rate':[0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "#imposto l'algoritmo di ricerca\n",
    "clf1 = GridSearchCV(GradientBoostingRegressor(random_state=10), \n",
    "                   param_grid = param_grid1, scoring='r2', \n",
    "                   cv=cv1).fit(X_train1, y_train1)\n",
    "\n",
    "#stampo i parametri migliori dell'algoritmo\n",
    "print(clf1.best_estimator_) \n",
    "#stampo il mioglior valore per il parametro voluto\n",
    "print(\"R Squared:\",clf1.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I valori ottenuti dopo la procedura di tuning sono conformi a quanto rilevato nelle tabelle precedenti.\n",
    "Usiamo la seguente funzione per rappresentare successivamente dei grafici (https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(0.1, 1.0, 10)):\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si inseriscono i valori dei parametri trovati nella fase di gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si modficano manualmente i valori dei parametri per alcuni test (clf1_test) per migliorare i risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test = GradientBoostingRegressor(learning_rate=0.1, max_depth=5,\n",
    "                                min_samples_split=10, n_estimators=100, \n",
    "                                random_state=10).fit(X_train, y_train)\n",
    "print(\"Test RMSE: \", np.sqrt(mean_squared_error(y_test, clf_test.predict(X_test))))\n",
    "print(\"Test R^2: \", r2_score(y_test, clf_test.predict(X_test)))\n",
    "print(\"Score in train:\", clf_test.score(X_train, y_train))\n",
    "print(\"Score in test:\", clf_test.score(X_test, y_test))\n",
    "clf_test.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si modficano manualmente i valori dei parametri per alcuni test (clf1_test) per migliorare i risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1_test = GradientBoostingRegressor(learning_rate=0.1, max_depth=5,\n",
    "                                min_samples_split=70, n_estimators=600, \n",
    "                                random_state=10).fit(X_train1, y_train1)\n",
    "print(\"Test RMSE: \", np.sqrt(mean_squared_error(y_test1, clf1_test.predict(X_test1))))\n",
    "print(\"Test R^2: \", r2_score(y_test1, clf1_test.predict(X_test1)))\n",
    "print(\"Score in train:\", clf1_test.score(X_train1, y_train1))\n",
    "print(\"Score in test:\", clf1_test.score(X_test1, y_test1))\n",
    "clf1_test.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come detto sopra, sono stati modificati i valori dei parametri riscontrati durante il processo di Grid search.\n",
    "I parametri forniti dalla procedura di ricerca e definiti come i migliori per il training producevano overfitting nella fase di test. Pertanto modificando opportunamente i valori, tra quelli inseriti nel grid search, si è ottenuto un risultato senza overfitting, dato che la combinazione dei valori usata non risulta quella migliore e quindi possiede un valore di R^2 inferiore a quello identificato dalla procedura.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si può osservare in entrambe le prove i valori di test di R^2 sembrano essere decisamente buoni oltre ad aver ridotto il valori di RMSE, tuttavia rispetto ai valori di training si evidenza il fenomeno di overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si considerano ora le curve di apprendimento in fase di training e di cross-validation e gli scatter plot true value vs predicted, per entrambi i casi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Curve di Apprendimneto (Gradient Boosted Regression)\" \n",
    "algorithm = GradientBoostingRegressor(learning_rate=0.1, max_depth=5,\n",
    "                                       min_samples_split=10, n_estimators=100, \n",
    "                                       random_state=10)\n",
    "cv=cv0\n",
    "plot_learning_curve(algorithm, title, X_train, y_train, ylim=(0.5, 1.01), cv=cv)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The line / model\n",
    "history=algorithm.fit(X_train,y_train)\n",
    "y_pred=algorithm.predict(X_test)\n",
    "plt.scatter(y_test, y_pred)\n",
    "x = np.linspace(0, 10, 1000)\n",
    "plt.plot(x, x + 0, '-g')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Curve di Apprendimneto (Gradient Boosted Regression)\" \n",
    "algorithm1 = GradientBoostingRegressor(learning_rate=0.1, max_depth=15,\n",
    "                                       min_samples_split=10, n_estimators=50, \n",
    "                                       random_state=10)\n",
    "cv=cv1\n",
    "plot_learning_curve(algorithm1, title, X_train1, y_train1, ylim=(0.5, 1.01), cv=cv)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The line / model\n",
    "algorithm1.fit(X_train1,y_train1)\n",
    "y_pred=algorithm1.predict(X_test1)\n",
    "plt.scatter(y_test1, y_pred)\n",
    "x = np.linspace(0, 100, 10000)\n",
    "plt.plot(x, x + 0, '-g')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confrontando i risultati relativi a R^2, nei rispettivi casi, con le curve sopra riportate si osserva che il mancato utilizzo della cross validation, in fase di training, porta ad un overfitting (confronto tra lo Score in test e l'ultimo valore della curva rossa), al contrario si osserva che lo score è superiore in fase di test (confronto Score in test con ultimo valore della curva verde).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per quanto riguarda gli scatter plot, si nota come il primo caso performi meglio del secondo, soprattutto verso valori più grandi. In generale però si osserva che i modelli effettuano buone previsioni, con la maggioranza dei punti che si collona intorno alla retta di identità."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo ora a visualizzare l'ordine di importanza delle features per il primo modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = algorithm.feature_importances_ \n",
    "print(importances)\n",
    "indices = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Covid19 - Top 6 Important Features\\n\") \n",
    "for f in range(6):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, X_df.columns[indices[f]], importances[indices[f]])) \n",
    "    #Plot the feature importances of the forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Mean Feature Importance %.6f\" %np.mean(importances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo un nuovo dataframe rimuovendo le features meno importanti, addestriamo un nuovo regressore sul dataframe creato e visualizziamo le differenti curve e scatterplot come effettuato in precedenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trimmed= X_df.copy()\n",
    "X_trimmed.drop(['HospitalizedPatients','IntensiveCarePatients', 'Recovered'], axis=1, inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df,train_size=0.7, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1, min_samples_leaf=9)\n",
    "estimator.fit(X_train, y_train) \n",
    "print (\"R-squared for Train: %.2f\" %estimator.score(X_train, y_train) )\n",
    "print (\"R-squared for Test: %.2f\" %estimator.score(X_test, y_test) )\n",
    "plot_learning_curve(estimator, title, X_train, y_train, cv=cv) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The line / model\n",
    "y_pred=estimator.predict(X_test)\n",
    "plt.scatter(y_test, y_pred)\n",
    "x = np.linspace(0, 10, 1000)\n",
    "plt.plot(x, x + 0, '-g')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche eliminando le features meno importanti, si osserva che il modello performa bene. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Forecasting <a name=\"forecasting\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa sezione proviamo ad effettuare il forecast sui valori di TotalPositiveCases e NewPositiveCases della regione maggiormente colpita, ovvero della Lombardia. In questa sezione proveremo a predire un arco temporale di 30 giorni, utilizzando una semplice tecnica di curve fitting, successivamente utilizzeremo tecniche più avanzate, come ad esempio i modelli ARIMA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo un dataframe contenente solo i dati relativi alla Lombardia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_stationary=ts_df.copy()\n",
    "temp_stationary=temp_stationary[temp_stationary.RegionName == 'Lombardia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_stationary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impostiamo come indice del dataframe la Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf=temp_stationary.copy()\n",
    "## convert index to datetime\n",
    "dtf.index = pd.to_datetime(dtf.Date, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo tre possibili funzioni, una funzione lineare, una esponenziale ed una gaussiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Linear function: f(x) = a + b*x\n",
    "'''\n",
    "def f(x):\n",
    "    return 10 + 1500*x\n",
    "\n",
    "y_linear = f(x=np.arange(len(dtf)))\n",
    "'''\n",
    "Exponential function: f(x) = a + b^x\n",
    "'''\n",
    "def f(x):\n",
    "    return 10 + 1.18**x\n",
    "\n",
    "y_exponential = f(x=np.arange(len(dtf)))\n",
    "'''\n",
    "Logistic function: f(x) = a / (1 + e^(-b*(x-c)))\n",
    "'''\n",
    "def f(x): \n",
    "    return 90000 / (1 + np.exp(-0.5*(x-20)))\n",
    "\n",
    "y_logistic = f(x=np.arange(len(dtf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizziamo in un unico grafico le tre funzioni sopra definite rispetto all'attuale andamento del totale dei positivi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13,5))\n",
    "ax.scatter(dtf[\"TotalPositiveCases\"].index, dtf[\"TotalPositiveCases\"].values, color=\"black\")\n",
    "ax.plot(dtf[\"TotalPositiveCases\"].index, y_linear, label=\"linear\", color=\"red\")\n",
    "ax.plot(dtf[\"TotalPositiveCases\"].index, y_exponential, label=\"exponential\", color=\"green\")\n",
    "ax.plot(dtf[\"TotalPositiveCases\"].index, y_logistic, label=\"logistic\", color=\"blue\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo a verificare se per la feature Nuovi Casi positivi, l'andamento che ci aspettiamo è quello di una gaussiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gaussian function: f(x) = a * e^(-0.5 * ((x-μ)/σ)**2)\n",
    "'''\n",
    "def f(x):\n",
    "    return 2000 * np.exp(-0.5 * ((x-24)/6)**2)\n",
    "\n",
    "y_gaussian = f(x=np.arange(len(dtf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13,5))\n",
    "ax.bar(dtf[\"NewPositiveCases\"].index, dtf[\"NewPositiveCases\"].values, color=\"black\")\n",
    "ax.plot(dtf[\"NewPositiveCases\"].index, y_gaussian, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificato che ci aspettiamo un andamento logistico per quanto riguarda il totale dei positivi e gaussiano per quanto riguarda il numero di nuovi positivi, procediamo con la definizione delle rispettive funzioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Logistic function: f(x) = capacity / (1 + e^-k*(x - midpoint) )\n",
    "'''\n",
    "def logistic_f(X, c, k, m):\n",
    "    y = c / (1 + np.exp(-k*(X-m)))\n",
    "    return y\n",
    "## optimize from scipy\n",
    "logistic_model, cov = optimize.curve_fit(logistic_f,\n",
    "                                np.arange(len(dtf[\"TotalPositiveCases\"])), \n",
    "                                dtf[\"TotalPositiveCases\"].values, \n",
    "                                maxfev=10000,\n",
    "                                p0=[np.max(dtf[\"TotalPositiveCases\"]), 1, 1])\n",
    "## print the parameters\n",
    "logistic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gaussian function: f(x) = a * e^(-0.5 * ((x-μ)/σ)**2)\n",
    "'''\n",
    "def gaussian_f(X, a, b, c):\n",
    "    y = a * np.exp(-0.5 * ((X-b)/c)**2)\n",
    "    return y\n",
    "## optimize from scipy\n",
    "gaussian_model, cov = optimize.curve_fit(gaussian_f,\n",
    "                                np.arange(len(dtf[\"NewPositiveCases\"])), \n",
    "                                dtf[\"NewPositiveCases\"].values, \n",
    "                                maxfev=10000,\n",
    "                                p0=[1, np.mean(dtf[\"NewPositiveCases\"]), 1])\n",
    "## print the parameters\n",
    "gaussian_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to fit. In this case logistic function:\n",
    "    f(x) = capacity / (1 + e^-k*(x - midpoint) )\n",
    "'''\n",
    "def f(X, c, k, m):\n",
    "    y = c / (1 + np.exp(-k*(X-m)))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizziamo le seguenti funzioni per definire il curve fitting ed effettuare il forecast seguendo l'andamento delle curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate dates to index predictions.\n",
    ":parameter\n",
    "    :param start: str - \"yyyy-mm-dd\"\n",
    "    :param end: str - \"yyyy-mm-dd\"\n",
    "    :param n: num - length of index\n",
    "    :param freq: None or str - 'B' business day, 'D' daily, 'W' weekly, 'M' monthly, 'A' annual, 'Q' quarterly\n",
    "'''\n",
    "def utils_generate_indexdate(start, end=None, n=None, freq=\"D\"):\n",
    "    if end is not None:\n",
    "        index = pd.date_range(start=start, end=end, freq=freq)\n",
    "    else:\n",
    "        index = pd.date_range(start=start, periods=n, freq=freq)\n",
    "    index = index[1:]\n",
    "    print(\"--- generating index date --> start:\", index[0], \"| end:\", index[-1], \"| len:\", len(index), \"---\")\n",
    "    return index\n",
    "'''\n",
    "Fits a custom function.\n",
    ":parameter\n",
    "    :param X: array\n",
    "    :param y: array\n",
    "    :param f: function to fit (ex. logistic: f(X) = capacity / (1 + np.exp(-k*(X - midpoint)))\n",
    "                                or gaussian: f(X) = a * np.exp(-0.5 * ((X-mu)/sigma)**2)   )\n",
    "    :param kind: str - \"logistic\", \"gaussian\" or None\n",
    "    :param p0: array or list of initial parameters (ex. for logistic p0=[np.max(ts), 1, 1])\n",
    ":return\n",
    "    optimal params\n",
    "'''\n",
    "def fit_curve(X, y, f=None, kind=None, p0=None):\n",
    "    ## define f(x) if not specified\n",
    "    if f is None:\n",
    "        if kind == \"logistic\":\n",
    "            f = lambda p,X: p[0] / (1 + np.exp(-p[1]*(X-p[2])))\n",
    "        elif find == \"gaussian\":\n",
    "            f = lambda p,X: p[0] * np.exp(-0.5 * ((X-p[1])/p[2])**2)\n",
    "    \n",
    "    ## find optimal parameters\n",
    "    model, cov = optimize.curve_fit(f, X, y, maxfev=10000, p0=p0)\n",
    "    return model\n",
    "    \n",
    "\n",
    "\n",
    "'''\n",
    "Predict with optimal parameters.\n",
    "'''\n",
    "def utils_predict_curve(model, f, X):\n",
    "    fitted = f(X, model[0], model[1], model[2])\n",
    "    return fitted\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Plot parametric fitting.\n",
    "'''\n",
    "def utils_plot_parametric(dtf, zoom=30, figsize=(15,5)):\n",
    "    ## interval\n",
    "    dtf[\"residuals\"] = dtf[\"ts\"] - dtf[\"model\"]\n",
    "    dtf[\"conf_int_low\"] = dtf[\"forecast\"] - 1.96*dtf[\"residuals\"].std()\n",
    "    dtf[\"conf_int_up\"] = dtf[\"forecast\"] + 1.96*dtf[\"residuals\"].std()\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=figsize)\n",
    "    \n",
    "    ## entire series\n",
    "    dtf[\"ts\"].plot(marker=\".\", linestyle='None', ax=ax[0], title=\"Parametric Fitting\", color=\"black\")\n",
    "    dtf[\"model\"].plot(ax=ax[0], color=\"green\", label=\"model\", legend=True)\n",
    "    dtf[\"forecast\"].plot(ax=ax[0], grid=True, color=\"red\", label=\"forecast\", legend=True)\n",
    "    ax[0].fill_between(x=dtf.index, y1=dtf['conf_int_low'], y2=dtf['conf_int_up'], color='b', alpha=0.3)\n",
    "   \n",
    "    ## focus on last\n",
    "    first_idx = dtf[pd.notnull(dtf[\"forecast\"])].index[0]\n",
    "    first_loc = dtf.index.tolist().index(first_idx)\n",
    "    zoom_idx = dtf.index[first_loc-zoom]\n",
    "    dtf.loc[zoom_idx:][\"ts\"].plot(marker=\".\", linestyle='None', ax=ax[1], color=\"black\", \n",
    "                                  title=\"Zoom on the last \"+str(zoom)+\" observations\")\n",
    "    dtf.loc[zoom_idx:][\"model\"].plot(ax=ax[1], color=\"green\")\n",
    "    dtf.loc[zoom_idx:][\"forecast\"].plot(ax=ax[1], grid=True, color=\"red\")\n",
    "    ax[1].fill_between(x=dtf.loc[zoom_idx:].index, y1=dtf.loc[zoom_idx:]['conf_int_low'], \n",
    "                       y2=dtf.loc[zoom_idx:]['conf_int_up'], color='b', alpha=0.3)\n",
    "    plt.show()\n",
    "    return dtf[[\"ts\",\"model\",\"residuals\",\"conf_int_low\",\"forecast\",\"conf_int_up\"]]\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Forecast unknown future.\n",
    ":parameter\n",
    "    :param ts: pandas series\n",
    "    :param f: function\n",
    "    :param model: list of optim params\n",
    "    :param pred_ahead: number of observations to forecast (ex. pred_ahead=30)\n",
    "    :param end: string - date to forecast (ex. end=\"2016-12-31\")\n",
    "    :param freq: None or str - 'B' business day, 'D' daily, 'W' weekly, 'M' monthly, 'A' annual, 'Q' quarterly\n",
    "    :param zoom: for plotting\n",
    "'''\n",
    "def forecast_curve(ts, f, model, pred_ahead=None, end=None, freq=\"D\", zoom=30, figsize=(15,5)):\n",
    "    ## fit\n",
    "    fitted = utils_predict_curve(model, f, X=np.arange(len(ts)))\n",
    "    dtf = ts.to_frame(name=\"ts\")\n",
    "    dtf[\"model\"] = fitted\n",
    "    \n",
    "    ## index\n",
    "    index = utils_generate_indexdate(start=ts.index[-1], end=end, n=pred_ahead, freq=freq)\n",
    "    \n",
    "    ## forecast\n",
    "    preds = utils_predict_curve(model, f, X=np.arange(len(ts)+1, len(ts)+1+len(index)))\n",
    "    dtf = dtf.append(pd.DataFrame(data=preds, index=index, columns=[\"forecast\"]))\n",
    "    \n",
    "    ## plot\n",
    "    utils_plot_parametric(dtf, zoom=zoom)\n",
    "    return dtf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effettuiamo il fitting ed il forecast basato sulla curva logistica per quanto riguarda il numero totale dei positivi in Lombardia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit\n",
    "model = fit_curve(X=np.arange(len(dtf[\"TotalPositiveCases\"])), y=dtf[\"TotalPositiveCases\"].values, f=f, p0=[np.max(dtf[\"TotalPositiveCases\"]), 1, 1])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forecast\n",
    "preds = forecast_curve(dtf[\"TotalPositiveCases\"], f, model, pred_ahead=30, freq=\"D\", zoom=7, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effettuiamo il fitting ed il forecast per quanto riguarda il numero dei nuovi casi positivi in Lombardia, basandoci sulla funzione gaussiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to fit. In this case gaussian function:\n",
    "    f(x) = a * e^(-0.5 * ((x-μ)/σ)**2)\n",
    "'''\n",
    "def f(X, a, b, c):\n",
    "    y = a * np.exp(-0.5 * ((X-b)/c)**2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_curve(X=np.arange(len(dtf[\"NewPositiveCases\"])), y=dtf[\"NewPositiveCases\"].values, f=f, p0=[1, np.mean(dtf[\"NewPositiveCases\"]), np.std(dtf[\"NewPositiveCases\"])])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forecast\n",
    "preds = forecast_curve(dtf[\"NewPositiveCases\"], f, model, pred_ahead=30, end=None, freq=\"D\", zoom=7, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Modello Logistico <a name=\"logistic\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa sezione verrà impiegato il modello logistico con lo scopo di valutare l'andamento dei contagi in Italia, al fine di definire un possibile periodo in cui ossevare una riduzione di questi ultimi.\n",
    "Il modello logistico è ampiamente utilizzato per descrivere la crescita di una popolazione. Un'infezione può essere descritta come la crescita della popolazione di un agente patogeno, quindi l'uso di un modello logistico sembra ragionevole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si procede con la selezione del dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tmp = data.copy()\n",
    "data_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene inserita una nuova feature relativa ai giorni trascorsi dall'inizio dell'anno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayCount=data_tmp.Date.count()\n",
    "ticks=range(0,dayCount,1)\n",
    "data_tmp['DayNumber']=ticks\n",
    "data_tmp['DayNumber']=data_tmp['DayNumber']+54\n",
    "data_tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eseguiamo una previsione sul numero di contagiati con il modello logistico, selezionando gli attributi opportuni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp = data_tmp.loc[:,['DayNumber','TotalPositiveCases']]\n",
    "data_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene implementata la formula logistica come segue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,timedelta\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import fsolve\n",
    "import datetime\n",
    "\n",
    "\n",
    "def logistic_model(x,a,b,c):\n",
    "    return c/(1+np.exp(-(x-b)/a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa formula, abbiamo la variabile x che è il tempo e tre parametri:\n",
    "- a si riferisce alla velocità dell'infezione\n",
    "- b è il giorno in cui si è verificato il massimo numero di infezioni\n",
    "- c è il numero totale di persone infette registrate alla fine dell'infezione\n",
    "\n",
    "Per valori temporali alti, il numero di persone infette si avvicina sempre di più a c e questo è il punto in cui possiamo dire che l'infezione è terminata. Questa funzione ha anche un punto di flesso in b, che è il punto in cui la prima derivata inizia a diminuire (cioè il picco dopo il quale l'infezione inizia a diventare meno aggressiva e pertanto diminuisce)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vengono stimati i valori dei parametri e gli errori a partire dai dati forniti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(data_temp.iloc[:,0])\n",
    "y = list(data_temp.iloc[:,1])\n",
    "\n",
    "fit = curve_fit(logistic_model,x,y,p0=[2,100,20000])\n",
    "fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I due array risultanti corrispondono rispettivamente a: \n",
    "- i parametri a, b, c\n",
    "- matrice della covarianza da cui estrarre gli errori\n",
    "    \n",
    "Gli errori vengono ottenuti tramite la radice quadrata dei valori posti sulla diaglonale della matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [np.sqrt(fit[1][i][i]) for i in [0,1,2]]\n",
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli errori ottenuti corrispondono agli errori standard dei parametri a, b e c, rispettivamente.\n",
    "\n",
    "Vengono inseriti i parametri all'interno della funzione di risoluzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=fit[0][0]\n",
    "b=fit[0][1]\n",
    "c=fit[0][2]\n",
    "sol = int(fsolve(lambda x : logistic_model(x,a,b,c) - int(c),b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si applica una modifica alla soluzione per ottenere una data relativa alla fine dei contagi.\n",
    "La fine dell'infezione può essere calcolata come il giorno in cui il valore cumulativo delle persone infette è uguale al parametro c arrotondato al numero intero più vicino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime(2020, 1, 1)\n",
    "dt = datetime.datetime(2020,1,1)\n",
    "dtdelta = datetime.timedelta(days=sol)\n",
    "d=dt + dtdelta\n",
    "print('',d.strftime(\"%A\"), '',d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si crea un grafico con l'andamento previsto rispetto a quello reale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = list(range(max(x),sol))\n",
    "plt.rcParams['figure.figsize'] = [7, 7]\n",
    "plt.rc('font', size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dati Reali\n",
    "plt.scatter(x,y,label=\"Dati reali\",color=\"red\")\n",
    "# Predizione della curva logistica\n",
    "plt.plot(x+pred_x, [logistic_model(i,fit[0][0],fit[0][1],fit[0][2]) for i in x+pred_x], label=\"Modello Logistico\" )\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Giorni dal 1 gennaio 2020\")\n",
    "plt.ylabel(\"Numero totale di persone infette\")\n",
    "plt.ylim((min(y)*0.9,c*1.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si osserva che la curva logistica sembra approssimare il comportamento dei dati reali in maniera adeguata.\n",
    "Tuttavia lo scopo di tale predizione è valutare un possibile periodo in cui potenzialmente si può avere una decrescita dei contagi.\n",
    "Stando alla situazione attuale, il periodo riscontrato può essere plausibile in quanto, attualmente, sembra in atto la decrescita delle persone infette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Modellazione Epidemiologica\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa sezione si vuole eseguire un analisi al fine di modellizzare la diffusione della malattia, sfruttando modelli epidemiologici noti in letteratura.\n",
    "I modelli considerati sono SIR e SEIR. Per quest'ultimo verrà analizzato anche il comportamento con diversi livelli di distanziamento sociale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Modello SIR <a name=\"sir\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello SIR è uno dei modelli compartimentali più semplici e molti modelli sono derivati da questa forma base.\n",
    "Il modello si compone di tre compartimenti S,I,R, dove assumendo N costante, si ha N=I(t)+S(t)+R(t).\n",
    "Ogni cmpartimento corrisponde a:\n",
    "- S, suscettibili;\n",
    "- I, infetti/infettivi;\n",
    "- R, recuperati, guariti, non infettabili perché immuni, dopo aver contratto la malattia. Alcuni autori nei loro modelli interpretano la R, come resistenti o rimossi, in quanto non partecipano al processo epidemico, immuni o isolati o deceduti. Quindi vengono inclusi in R(t) anche i morti dovuti alla malattia.\n",
    "\n",
    "L'evoluzione della malattia è generalmente formulata sotto forma di equazioni differenziali ordinarie (ODE).\n",
    "I parametri in gioco, per questo modello, sono i seguenti:\n",
    "- R con zero (riportato come 'r' nel codice)--> definito anche come numero di riproduzione, indica la potenziale trasmissibilità di una malattia infettiva. Più precisamente esso rappresenta il numero di nuovi casi generati, in media, da un singolo caso durante il proprio periodo infettivo, in una popolazione che altrimenti non sarebbe infetta. Questo parametro risulta relazionato con altri parametri tramite la seguente relazione: R=beta/gamma.\n",
    "- beta --> corrisponde al tasso di contatto nella popolazione.\n",
    "- gamma --> corrisponde all'inverso del periodo infettivo.\n",
    "\n",
    "Per il primo parametro, il valore è stato ricavato da [1]\n",
    "\n",
    "Per quanto riguarda gamma, il valore è stato prelevato da [2]\n",
    "\n",
    "Si precisa che tutti i parametri risultano contestualizzati con i dati relativi al periodo in esame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import odeint\n",
    "\n",
    "#R0\n",
    "r=3.5\n",
    "# Popolazione totale, N.\n",
    "N = 60361700\n",
    "# infetti all'inizio=I0 e recuperati=R0.\n",
    "I0, R0 = (data_tmp['TotalPositiveCases'].min()-data_tmp['Deaths'].min()-data_tmp['Recovered'].min()), data_tmp['Deaths'].min()+data_tmp['Recovered'].min()\n",
    "# Chiunque (S0) è suscettibile alla malattia, all'inizio.\n",
    "S0 = N - I0 - R0\n",
    "# beta: tasso di contatto nella popolazione sufficiente per la trasmissione della malattia\n",
    "# gamma: inverso del tempo di infezione\n",
    "beta, gamma = r*0.5, 0.5\n",
    "# A grid of time points (in days)\n",
    "t = np.linspace(0, 60, 60)\n",
    "\n",
    "# modello SIR\n",
    "def deriv(y, t, N, beta, gamma):\n",
    "    S, I, R = y\n",
    "    dSdt = -beta * S * I / N\n",
    "    dIdt = beta * S * I / N - gamma * I\n",
    "    dRdt = gamma * I\n",
    "    return dSdt, dIdt, dRdt\n",
    "\n",
    "# vettore con le condizioni iniziali\n",
    "y0 = S0, I0, R0\n",
    "# Integrazione dell'equazione del modello nel tempo.\n",
    "ret = odeint(deriv, y0, t, args=(N, beta, gamma))\n",
    "S, I, R = ret.T\n",
    "\n",
    "# Stampo i grafici delle tre curve S(t), I(t) e R(t)\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.plot(t,S,label='Suscettibili')\n",
    "plt.plot(t,I,label='Infetti')\n",
    "plt.plot(t,R,label='Guariti/Recuperati')\n",
    "plt.legend()\n",
    "plt.xlabel('Tempo/giorni')\n",
    "plt.ylabel('Numero soggetti')\n",
    "plt.title('Modello SIR')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dal grafico si osserva che la naturale diffusione della malattia, senza alcuna forma di controllo, considerando anche il fatto di avere una propagazione più rapida rispetto a quello che può essere un adeguato rilevamento e intervento degli organi sanitari, comporta il contagio di circa un terzo della popolazione all'interno di un breve lasso temporale.\n",
    "Tale considerazione risulta concorde con le ipotesi rilasciate nel periodo contestuale ai dati. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Modello SEIR <a name=\"seir\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per molte importanti infezioni c'è un significativo periodo di incubazione durante il quale gli individui sono stati infettati, ma non sono ancora infettivi. Durante questo periodo l'individuo si trova nel compartimento E che sta per \"esposto\". Supponendo che il periodo di incubazione sia una variabile casuale con distribuzione esponenziale con parametro alfa, considerando il periodo medio di incubazione come 1/alfa, con N costante, si ottiene il modello di seguito implementato.\n",
    "Come per i precedenti parametri anche il valore di alfa compare nell'articolo sopra proposto.\n",
    "\n",
    "Anche in questo caso si osserva l'impiego di equazioni diferenziali ordinarie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_seir_model(init_vals, params, t):\n",
    "    S_0, E_0, I_0, R_0 = init_vals\n",
    "    S, E, I, R = [S_0], [E_0], [I_0], [R_0]\n",
    "    alpha, beta, gamma = params\n",
    "    dt = t[1] - t[0]\n",
    "    for _ in t[1:]:\n",
    "        next_S = S[-1] - (beta*S[-1]*I[-1])*dt\n",
    "        next_E = E[-1] + (beta*S[-1]*I[-1] - alpha*E[-1])*dt\n",
    "        next_I = I[-1] + (alpha*E[-1] - gamma*I[-1])*dt\n",
    "        next_R = R[-1] + (gamma*I[-1])*dt\n",
    "        S.append(next_S)\n",
    "        E.append(next_E)\n",
    "        I.append(next_I)\n",
    "        R.append(next_R)\n",
    "    return np.stack([S, E, I, R]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito vengono definiti i parametri da applicare al modello, con successiva rappresentazione grafica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameteri\n",
    "t_max = 100\n",
    "dt = 0.1\n",
    "t = np.linspace(0, t_max, int(t_max/dt) + 1)\n",
    "N = 60361700\n",
    "init_vals = 1-((data_tmp['TotalPositiveCases'].min()-data_tmp['Deaths'].min()-data_tmp['Recovered'].min())/N), 0, (data_tmp['TotalPositiveCases'].min()-data_tmp['Deaths'].min()-data_tmp['Recovered'].min())/N, (data_tmp['Deaths'].min()+data_tmp['Recovered'].min())/N\n",
    "alpha = 0.2\n",
    "beta = 1.75 # ricordiamo r=beta/gamma con r=3,5\n",
    "gamma = 0.5\n",
    "params = alpha, beta, gamma\n",
    "\n",
    "results = base_seir_model(init_vals, params, t)\n",
    "S, E, I, R=results.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "plt.plot(t,S,label='Suscettibili')\n",
    "plt.plot(t,E,label='Esposti')\n",
    "plt.plot(t,I,label='Infetti')\n",
    "plt.plot(t,R,label='Guariti/Recuperati')\n",
    "plt.legend()\n",
    "plt.xlabel('Tempo (giorni)')\n",
    "plt.ylabel('%Numero soggetti')\n",
    "plt.title('Modello SEIR')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si osserva che in questo caso, senza distanziamento sociale o altra forma di prevenzione, al culmine, il 10% della popolazione risulta infettata dalla malattia dopo circa 50 giorni dalla prima esposizione, con alte probabilità di essere un'infezione grave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da quanto osservato nei grafici emerge che una percentuale rilevante della popolazione risulta essere potenzialmente colpita. Tali valori possono essere coerenti con il ritardo degli interventi di rilevamento, date le scarse informazioni note sulla malattia in quel periodo, ma soprattutto per il fatto che il virus è rimasto latente e sottotraccia per un lungo periodo di tempo. Quindi i dati noti rappresentano solo una parte di quella che era la situazione nel contesto analizzato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Modello SEIR con distanziamento sociale <a name=\"seird\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si considera ora il modello SEIR, cercando di modellare l'effetto del distanziamento o di altre forme di prevenzione.\n",
    "Nel modello infatti si cerchera di agire sul parametro beta, introducento un parametro rho che tenga conto degli effetti della prevenzione. Rho è un termine costante tra 0 e 1, dove 0 indica un blocco totale di ogni attività, mentre 1 equivale al caso sopra esposto (con assenza di forme preventive). Si procede pertanto moltiplicando beta per il nuovo parametro inserito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seir_model_with_soc_dist(init_vals, params, t):\n",
    "    S_0, E_0, I_0, R_0 = init_vals\n",
    "    S, E, I, R = [S_0], [E_0], [I_0], [R_0]\n",
    "    alpha, beta, gamma, rho = params\n",
    "    dt = t[1] - t[0]\n",
    "    for _ in t[1:]:\n",
    "        next_S = S[-1] - (rho*beta*S[-1]*I[-1])*dt\n",
    "        next_E = E[-1] + (rho*beta*S[-1]*I[-1] - alpha*E[-1])*dt\n",
    "        next_I = I[-1] + (alpha*E[-1] - gamma*I[-1])*dt\n",
    "        next_R = R[-1] + (gamma*I[-1])*dt\n",
    "        S.append(next_S)\n",
    "        E.append(next_E)\n",
    "        I.append(next_I)\n",
    "        R.append(next_R)\n",
    "    return np.stack([S, E, I, R]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vengono eseguiti due valutazioni comparandole con il caso base.\n",
    "I valori per rho sono quindi: 1, 0.5 e 0.35. Successivamente vengono rappresentate le curve degli infetti e degli esposti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_m = 400\n",
    "dt = 0.1\n",
    "t1 = np.linspace(0, t_m, int(t_m/dt) + 1)\n",
    "\n",
    "rho1= 1\n",
    "rho2= 0.5\n",
    "rho3 = 0.35\n",
    "\n",
    "params1 = alpha, beta, gamma, rho1\n",
    "results1 = seir_model_with_soc_dist(init_vals, params1, t1)\n",
    "S1, E1, I1, R1=results1.T\n",
    "\n",
    "params2 = alpha, beta, gamma, rho2\n",
    "results2 = seir_model_with_soc_dist(init_vals, params2, t1)\n",
    "S2, E2, I2, R2=results2.T\n",
    "\n",
    "params3 = alpha, beta, gamma, rho3\n",
    "results3 = seir_model_with_soc_dist(init_vals, params3, t1)\n",
    "S3, E3, I3, R3=results3.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.grid()\n",
    "\n",
    "#plt.plot(t1,S1,label='Suscettibili')\n",
    "plt.plot(t1,E1,label='Esposti')\n",
    "plt.plot(t1,I1,label='Infetti')\n",
    "#plt.plot(t1,R1,label='Guariti')\n",
    "\n",
    "#plt.plot(t1,S2,'--',label='Suscettibili')\n",
    "plt.plot(t1,E2,'--',label='Esposti')\n",
    "plt.plot(t1,I2,'y--',label='Infetti')\n",
    "#plt.plot(t1,R2,'k--',label='Guariti')\n",
    "\n",
    "#plt.plot(t1,S3,'c:',label='Suscettibili')\n",
    "plt.plot(t1,E3,':',label='Esposti')\n",
    "plt.plot(t1,I3,'m:',label='Infetti')\n",
    "#plt.plot(t1,R3,':',label='Guariti/Recuperati')\n",
    "\n",
    "def connectpoints(x,y,p1,p2,p3):\n",
    "    x1, x2, x3 = x[p1], x[p2], x[p3]\n",
    "    y1, y2, y3 = y[p1], y[p2], y[p3]\n",
    "    plt.plot([x1,x2,x3],[y1,y2,y3],'k-')\n",
    "    \n",
    "time = [50, 123, 320]\n",
    "perc = [0.10, 0.031, 0.005]\n",
    "\n",
    "for i in range(0, len(time)):\n",
    "    plt.plot(time[i], perc[i], 'ro-')\n",
    "\n",
    "i=0\n",
    "connectpoints(time,perc,i,i+1,i+2)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.xlabel('Tempo (giorni)')\n",
    "plt.ylabel('%Numero soggetti')\n",
    "plt.title('Modello SEIR')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si può notare un effetto di appiattimento nelle curve degli infetti e degli esposti, con l'aumentare delle restrizioni (quindi riducendo il parametro rho).\n",
    "Le azioni di distanziamento e prevenzione sembrano mostrare un contenimento dei contagi nei primi periodi, spostando il picco degli infetti avanti nel tempo, permettendo la preparazione di pratiche di adeguamento sociale e sanitario. Considerando la successione dei picchi, con diversi valori di rho, si nota che la decrescita sembra esponenziale.\n",
    "La presenza di picchi relativi agli infetti ed agli esposti, nelle curve relative al distanziamento, è dovuta al fatto che il virus riduce la sua interazione con i soggetti, senza comunque essere eliminato, in quanto resta latente nell'ambiente.\n",
    "\n",
    "L'analisi con distanziamento sociale mette pertanto in luce la possibilità di miglioramenti nell'ambito della sopravvivenza alla malattia, fornendo più tempo per lo sviluppo di trattamenti e cure, con un mantenimento dei picchi più bassi.\n",
    "Quanto detto sembra essere conforme a quanto accade oggi in Italia, dove si evidenzia un calo degli infetti grazie al periodo di lockdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SARIMAX <a name=\"arima\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo ad effettuare forecast più precisi rispetto al semplice curve fitting, utilizzando il metodo SARIMAX. In questo caso creiamo un dataframe specifico, composto dall'indice che sarà la data e dalla feature di cui intediamo effettuare la predizione. Il dataframe creato possiede come feature il numero totale di positivi a livello italiano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_date):\n",
    "    return data[data.index <= split_date].copy(), \\\n",
    "           data[data.index >  split_date].copy()\n",
    "\n",
    "def limit(data, frm, to):\n",
    "    return data[(data.index>=frm)&(data.index<to)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data_tmp.copy()\n",
    "t.set_index('Date', inplace=True) \n",
    "t.index = pd.to_datetime(t.index)\n",
    "t.drop(['TotalHospitalizedPatients','DayNumber', 'Recovered', 'Deaths', 'TestsPerformed', 'HospitalizedPatients'], axis=1, inplace=True)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effettuiamo una decomposizione della serie per visualizzare i trend e la stagionalità, nonchè i residui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(t, model='additive', freq=7)\n",
    "fig = result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifichiamo se la serie sia stazionaria o non stazionaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns # data visualisation\n",
    "import matplotlib.pyplot as plt # data visualisation\n",
    "import datetime as dt # working with time data\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "results = adfuller(t[\"TotalPositiveCases\"])\n",
    "print('ADF Statistic: %f' % results[0])\n",
    "print('p-value: %f' % results[1])\n",
    "print('Critical Values:')\n",
    "for key, value in results[4].items():\n",
    "    print('\\t{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La serie sembra stazionaria, considerando ADF e p-value rispetto ai valori critici.\n",
    "\n",
    "Visualizziamo l'autocorrelazione e l'autocorrelazione parziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "fig,ax = plt.subplots(2,1,figsize=(20,10))\n",
    "plot_acf(t, lags=27, ax=ax[0])\n",
    "plot_pacf(t, lags=27, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax = lag_plot(t, lag=1)\n",
    "ax = lag_plot(t, lag=2, c=\"orange\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tramite la visualizzazione delle varie metriche, come Akaike Information Critera (AIC) and Bayesian Information Criteria (BIC), proviamo a trovare la migliore combinazione di parametri per addestrare il modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "arima_df = pd.DataFrame(columns=[\"p\",\"q\",\"aic\",\"bic\"])\n",
    "\n",
    "i=0\n",
    "# Loop over p values from 0-3\n",
    "for p in range(6):\n",
    "    # Loop over q values from 0-3\n",
    "    for q in range(6):\n",
    "        \n",
    "        try:\n",
    "            # creating and fitting ARIMA(p,1,q) model\n",
    "            model = ARIMA(t.astype(float), order=(p,1,q))\n",
    "            results = model.fit()\n",
    "            \n",
    "            # Printing order, AIC and BIC\n",
    "            #print(p, q, results.aic, results.bic)\n",
    "            arima_df.loc[i,\"p\"] = p\n",
    "            arima_df.loc[i,\"q\"] = q\n",
    "            arima_df.loc[i,\"aic\"] = results.aic\n",
    "            arima_df.loc[i,\"bic\"] = results.bic\n",
    "            i = i+1\n",
    "        except:\n",
    "            #print(p, q, None, None)\n",
    "            i = i+1\n",
    "    \n",
    "arima_df[\"sum_aic_bic\"] = arima_df[\"aic\"]+arima_df[\"bic\"]\n",
    "arima_df.sort_values(by=\"sum_aic_bic\", ascending=False, inplace=True)\n",
    "arima_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo che la migliore combinazione è quella data da p=1 ed q=0, dal momento che è la combinazione che possiede il valore AIC minore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addestriamo il modello sarimax utilizzando la suddetta combinazione di parametri e ne stampiamo le varie metriche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "model2 = SARIMAX(t, order=(1,1,0), seasonal_order=(0,1,0,12))\n",
    "results = model2.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 12, 8\n",
    "plot = results.plot_diagnostics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalle metriche e dai plot effettuati notiamo che il modello è abbastanza affidabile, infatti ad esempio i quantili si distribuiscono bene intorno alla retta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effettuiamo il forecast del valore di TotalPositiveCases per i successivi 10 giorni rispetto all'ultima data presente nel nostro dataset, ne mostriamo i valori in una tabella riassuntiva ed infine ne mostriamo i grafici con i rispettivi intervalli di confidenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SARIMA mean forecast\n",
    "forecast = results.get_forecast(steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: since we did not specify the alpha parameter, the\n",
    "# confidence level is at the default, 95%\n",
    "print(forecast.summary_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endog = t['TotalPositiveCases']\n",
    "endog.plot(figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "# Construct the model\n",
    "mod = sm.tsa.SARIMAX(endog, order=(2, 1, 0))\n",
    "# Estimate the parameters\n",
    "res = mod.fit()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "# Plot the data (here we are subsetting it to get a better look at the forecasts)\n",
    "#endog.plot(ax=ax)\n",
    "\n",
    "# Construct the forecasts\n",
    "fcast = res.get_forecast(steps=10).summary_frame()\n",
    "fcast['mean'].plot(ax=ax, style='k--')\n",
    "ax.fill_between(fcast.index, fcast['mean_ci_lower'], fcast['mean_ci_upper'], color='k', alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo come nei primi giorni di predizione l'intervallo di confidenza sia ridotto, ed aumenti man mano che si effettuano predizioni più avanti nel tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Altri modelli per Forecasting <a name=\"otherforecasting\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo ad effettuare il forecast, sempre sul numero totale di positivi a livello italiano, utilizzando ulteriori modelli, quali KernelRidge, SVR e MLPRegressor, mostrandone i grafici comparativi di predizioni e attuali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR, SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import KFold, train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def tsplit(X,y,model):\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    fig,ax = plt.subplots(3, figsize=(15,8))\n",
    "    axis = 0\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        #splitting data into training and test sets\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y.iloc[train_index,:], y.iloc[test_index,:]\n",
    "        #fitting model\n",
    "        model.fit(X_train,y_train.values.ravel())\n",
    "        #predicting\n",
    "        predictions = model.predict(X_test)\n",
    "        #printing results\n",
    "        print(\"MSE for split {0}:\".format(axis+1))\n",
    "        print(mean_squared_error(y_test,predictions))\n",
    "        #ax[axis].plot(X_train.index, y_train) # needs fixing\n",
    "        ax[axis].plot(list(X_test.index), predictions)\n",
    "        ax[axis].plot(list(X_test.index), y_test)\n",
    "        axis += 1\n",
    "    return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data_tmp.copy()\n",
    "t.drop(['TotalHospitalizedPatients','Date', 'Recovered', 'Deaths', 'TestsPerformed', 'HospitalizedPatients'], axis=1, inplace=True)\n",
    "t.shape\n",
    "X = t[[\"DayNumber\"]]\n",
    "y = t[[\"TotalPositiveCases\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(t[\"DayNumber\"],t[\"TotalPositiveCases\"],c=t[\"DayNumber\"])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc_x = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "# Scale x and y (two scale objects)\n",
    "X2_scaled = pd.DataFrame(sc_x.fit_transform(X))\n",
    "y2_scaled = pd.DataFrame(sc_y.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdg = KernelRidge(kernel='rbf')\n",
    "parameters = {'alpha':np.arange(0.005,0.02,0.005), 'gamma':np.arange(0.001,0.01,0.001)}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "rdg_gs = GridSearchCV(rdg, parameters, cv=tscv, verbose=0, scoring='neg_mean_squared_error')\n",
    "rdg_gs.fit(X2_scaled, y2_scaled)\n",
    "\n",
    "rdg_gs.best_score_\n",
    "best_rdg = rdg_gs.best_estimator_\n",
    "print(best_rdg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplit(X2_scaled,y2_scaled,best_rdg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR()\n",
    "parameters = {'kernel':['rbf','poly'],\n",
    "              'C':np.arange(0.2,0.8,0.1),\n",
    "              'gamma':np.arange(0.2,1.2,0.02),\n",
    "              'degree':[3,4,5]}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "reg = GridSearchCV(svr, parameters, cv=tscv, verbose=0, scoring='neg_mean_squared_error')\n",
    "reg.fit(X2_scaled, y2_scaled.values.ravel())\n",
    "\n",
    "reg.best_score_\n",
    "best_svr = reg.best_estimator_\n",
    "print(best_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplit(X2_scaled,y2_scaled,best_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "mlp = MLPRegressor(max_iter=600)\n",
    "parameters = {'hidden_layer_sizes':np.arange(800,1400,50),'alpha':[0.0001,0.0002], 'momentum':[0.85,0.9,0.95]}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "reg = GridSearchCV(mlp, parameters, cv=tscv, verbose=0, scoring='neg_mean_squared_error')\n",
    "reg.fit(X2_scaled, y2_scaled.values.ravel())\n",
    "\n",
    "reg.best_score_\n",
    "best_mlp = reg.best_estimator_\n",
    "print(best_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplit(X2_scaled,y2_scaled,best_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dai grafici notiamo che l'unico modello che approssima con un buon grado i valori reali con quelli predetti è il MLPRegressor, mentre nel KernelRidge e nel SVR abbiamo risultati non convincenti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References <a name=\"references\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] R0 modello sir: http://www.salute.gov.it/portale/news/p3_2_1_1_1.jsp?lingua=italiano&menu=notizie&p=dalministero&id=4429\n",
    "\n",
    "[2] Gamma modello sir: https://www.medrxiv.org/content/10.1101/2020.04.17.20053157v1.full.pdf\n",
    "\n",
    "[3] https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
